<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Master Python programming Course</title>
    <link rel="shortcut icon" type="png" href="logoG copy.png">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.11.4/gsap.min.js"></script>
    <link href="https://maxcdn.bootstrapcdn.com/bootstrap/5.3.0/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <style>
        /* 10 buttons start */




        .contentone-wrapperrr {
            background-color: #ffffff;
            border-radius: 12px;
            width: 100%;
            /* Full width of the screen */
            padding-bottom: 20px;
            position: relative;
            overflow-x: auto;
            /* Allows horizontal scrolling if needed */
        }

        /* Hide scrollbar for Webkit browsers (Chrome, Safari) */
        .contentone-wrapperrr::-webkit-scrollbar {
            display: none;
            /* Hides the scrollbar */
        }

        /* Hide scrollbar for Firefox */
        .contentone-wrapperrr {
            scrollbar-width: none;
            /* Hides the scrollbar */
        }

        /* Hide scrollbar for Internet Explorer and Edge */
        .contentone-wrapperrr {
            -ms-overflow-style: none;
            /* Hides the scrollbar */
        }

        .content-header {
            background-color: var(--header-background);
            color: #ffffff;
            padding: 10px;
            font-size: 18px;
            font-weight: bold;
            text-align: center;
        }

        .grid-layout {
            display: flex;
            flex-wrap: nowrap;
            /* Keeps all items in a single line */
            justify-content: space-between;
            /* Distributes items evenly across the line */
            gap: 20px;
            /* Adds spacing between the items */
            padding: 15px;
            width: 100%;
            /* Ensures the layout takes up the full width of the screen */
            box-sizing: border-box;
            /* Includes padding in the width calculation */
        }

        .item-card {
            background-color: #f8f9fa;
            border-radius: 8px;
            padding: 8px;
            cursor: pointer;
            transition: background-color 0.3s ease;
            display: flex;
            flex-direction: column;
            align-items: center;
            text-align: center;
            flex: 0 0 auto;
            /* Prevents items from shrinking or growing */
            max-width: 120px;
            /* Controls the maximum width of each item */
            min-width: 120px;
            /* Ensures a minimum width for each item */
        }

        .item-card:hover {
            background-color: #e9ecef;
        }

        .card-icon {
            font-size: 22px;
            color: var(--icon-color);
            margin-bottom: 5px;
        }

        .card-title {
            font-weight: bold;
            font-size: 14px;
            color: #002366;
            /* Dark blue text color */
        }

        /* Responsive adjustments */
        @media (max-width: 1200px) {
            .grid-layout {
                gap: 15px;
                /* Reduce gap on smaller screens */
            }
        }

        @media (max-width: 992px) {
            .grid-layout {
                gap: 10px;
                /* Further reduce gap on even smaller screens */
            }
        }

        @media (max-width: 768px) {
            .grid-layout {
                gap: 5px;
                /* Minimal gap on tablets */
            }
        }

        @media (max-width: 480px) {
            .grid-layout {
                gap: 5px;
                /* Minimal gap on small screens */
                padding: 10px;
                /* Adjust padding for smaller screens */
            }

            .item-card {
                max-width: 100px;
                /* Adjust max-width for smaller screens */
                min-width: 100px;
                /* Adjust min-width for smaller screens */
            }
        }



        /* 10 buttons end */


        /* Style for the loading screen */
        #loading-screen {
            position: fixed;
            width: 100%;
            height: 100%;
            background-color: #ffffff;
            /* Background color of the loading screen */
            display: flex;
            justify-content: center;
            align-items: center;
            z-index: 1000;
            /* Ensures it stays on top */
        }

        /* Simple loading icon style */
        .loading-icon {
            border: 8px solid #f3f3f3;
            /* Light grey */
            border-top: 8px solid #3498db;
            /* Blue */
            border-radius: 50%;
            width: 60px;
            height: 60px;
            animation: spin 1s linear infinite;
        }

        /* Keyframes for the spinning animation */
        @keyframes spin {
            0% {
                transform: rotate(0deg);
            }

            100% {
                transform: rotate(360deg);
            }
        }

        /* Style for the main content once loaded */
        #main-content {
            display: none;
            /* Initially hidden until loading is complete */
        }
    </style>
    <style>
        /* Your existing CSS styles */
        :root {
            --primary-color: #2c3e50;
            --secondary-color: #34495e;
            --accent-color: #e74c3c;
            --text-color: #ecf0f1;
            --background-color: #f9f9f9;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background-color: var(--background-color);
            color: var(--primary-color);
            line-height: 1.6;
        }

        .header {
            background: rgb(0, 0, 0);
            color: var(--text-color);
            padding: 2rem;
            text-align: center;
            position: relative;
            overflow: hidden;
        }

        .code-background {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            opacity: 0.1;
            font-family: monospace;
            font-size: 14px;
            overflow: hidden;
            white-space: nowrap;
        }

        .header-content {
            position: relative;
            z-index: 1;
        }

        .header-title {
            font-size: 3rem;
            font-weight: bold;
            margin-bottom: 0.5rem;
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3);
        }

        .header-subtitle {
            font-size: 1.5rem;
            margin-bottom: 1rem;
            opacity: 0.9;
        }

        .cta-button {
            display: inline-block;
            background-color: var(--accent-color);
            color: var(--text-color);
            padding: 0.8rem 1.5rem;
            border-radius: 30px;
            text-decoration: none;
            font-weight: bold;
            transition: all 0.3s ease;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        }

        .cta-button:hover {
            background-color: #c0392b;
            transform: translateY(-2px);
            box-shadow: 0 6px 8px rgba(0, 0, 0, 0.15);
        }

        .container {
            max-width: 1200px;
            margin: 2rem auto;
            padding: 0 1rem;
        }

        .nav-container {
            display: flex;
            justify-content: space-between;
            margin-bottom: 2rem;
        }

        .nav-box {
            flex: 1;
            background-color: #fff;
            border-radius: 10px;
            padding: 1.5rem;
            text-align: center;
            cursor: pointer;
            transition: all 0.3s ease;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            margin: 0 0.5rem;
        }

        .nav-box:hover {
            transform: translateY(-5px);
            box-shadow: 0 6px 8px rgba(0, 0, 0, 0.15);
        }

        .nav-icon {
            font-size: 2rem;
            margin-bottom: 0.5rem;
            color: var(--primary-color);
        }

        .main-content {
            background-color: #fff;
            border-radius: 10px;
            padding: 2rem;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            display: none;
        }

        .main-content.active {
            display: block;
        }

        .content-item {
            background-color: #f1f1f1;
            border-radius: 8px;
            margin-bottom: 1rem;
            overflow: hidden;
        }

        .content-title {
            background-color: var(--secondary-color);
            color: var(--text-color);
            padding: 1rem;
            cursor: pointer;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .content-body {
            padding: 1rem;
        }

        .chapter-list {
            list-style-type: none;
            padding: 0;
        }

        .chapter-item {
            background-color: #fff;
            border-radius: 8px;
            margin-bottom: 1rem;
            overflow: hidden;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        }

        .chapter-header {
            background-color: var(--secondary-color);
            color: var(--text-color);
            padding: 1rem;
            cursor: pointer;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .chapter-content {
            padding: 1rem;
            display: none;
        }

        .chapter-content ul {
            list-style-type: none;
            padding-left: 1rem;
        }

        .chapter-content li {
            margin-bottom: 0.5rem;
        }

        .faq-item {
            background-color: #fff;
            border-radius: 8px;
            margin-bottom: 1rem;
            overflow: hidden;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        }

        .faq-question {
            background-color: var(--secondary-color);
            color: var(--text-color);
            padding: 1rem;
            cursor: pointer;
        }

        .faq-answer {
            padding: 1rem;
            display: none;
        }
        

        .premium-overlay {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(0, 0, 0, 0.7);
            display: none;
            justify-content: center;
            align-items: center;
            z-index: 1000;
        }

        .premium-content {
            background-color: #fff;
            border-radius: 10px;
            padding: 1.5rem;
            text-align: center;
            max-width: 400px;
            /* Reduced width */
            width: 90%;
            /* Adjusted width */
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        }

        .premium-title {
            font-size: 1.75rem;
            /* Slightly smaller font size */
            margin-bottom: 1rem;
        }

        .premium-price {
            font-size: 1.25rem;
            /* Slightly smaller font size */
            color: var(--accent-color);
            margin-bottom: 1rem;
        }

        .premium-features {
            list-style-type: none;
            margin: 0;
            padding: 0;
            margin-bottom: 1rem;
        }

        .premium-features li {
            margin-bottom: 0.5rem;
            display: flex;
            align-items: center;
            /* Center icons and text vertically */
            font-size: 1rem;
            /* Adjusted font size */
        }

        .premium-features i {
            margin-right: 10px;
            /* Space between icon and text */
            color: var(--accent-color);
            /* Color for icons */
        }

        .premium-overlay .cta-button {
            background-color: var(--primary-color);
            color: var(--text-color);
            padding: 0.7rem 1.2rem;
            /* Slightly smaller button */
            border-radius: 25px;
            /* Rounded corners */
            font-weight: bold;
            border: none;
            cursor: pointer;
            transition: all 0.3s ease;
            margin: 0.5rem;
            font-size: 1rem;
            /* Adjusted font size */
        }

        .premium-overlay .cta-button:hover {
            background-color: #1a252f;
        }


        .close-button {
            background: var(--accent-color);
            color: var(--text-color);
            border: none;
            padding: 0.5rem 1rem;
            border-radius: 20px;
            cursor: pointer;
            font-weight: bold;
            transition: all 0.3s ease;
        }

        .close-button:hover {
            background-color: #c0392b;
        }

        @media (max-width: 768px) {
            .nav-container {
                flex-direction: column;
            }

            .nav-box {
                margin: 0.5rem 0;
            }

            .header-title {
                font-size: 2rem;
            }

            .header-subtitle {
                font-size: 1rem;
            }
        }
    </style>
    <style>
        .chapter-header {
            display: flex;
            justify-content: space-between;
            cursor: pointer;
            font-weight: bold;
        }

        .chapter-content {
            padding: 10px;
            border: 1px solid #ddd;
            margin-top: 10px;
            background-color: #f9f9f9;
        }

        .solution {
            background-color: #eee;
            padding: 10px;
            border-radius: 5px;
            margin-top: 10px;
        }
    </style>
</head>

<body>
    <!-- Loading screen -->
    <div id="loading-screen">
        <!-- Loading icon -->
        <div class="loading-icon"></div>
    </div>

    <!-- Main content -->
    <div id="main-content">
        <!-- Your website content goes here -->

    </div>

    <script>
        document.addEventListener("DOMContentLoaded", function () {
            // Simulate a loading process with a timeout
            setTimeout(function () {
                // Hide the loading screen
                document.getElementById('loading-screen').style.display = 'none';
                // Show the main content
                document.getElementById('main-content').style.display = 'block';
            }, 2000); // Adjust the time (in milliseconds) as needed
        });
    </script>
    <header class="header">
        <div class="code-background" id="codeBackground"></div>
        <div class="header-content">
            <h1 class="header-title">Master Machine Learning Course</h1>
            <p class="header-subtitle">Elevate Your Engineering Skills</p>
            <a href="#" class="cta-button" id="ctaButton">Get Premium Access</a>
        </div>
    </header>

    <div class="container">
        <div class="nav-container">
            <div class="nav-box" data-content="introduction">
                <div class="nav-icon"><i class="fas fa-book"></i></div>
                <div>Introduction</div>
            </div>
            <div class="nav-box" data-content="chapters">
                <div class="nav-icon"><i class="fas fa-list"></i></div>
                <div>Chapters</div>
            </div>
            <div class="nav-box" data-content="case-study">
                <div class="nav-icon"><i class="fas fa-briefcase"></i></div>
                <div>Case Study</div>
            </div>
            <div class="nav-box" data-content="faqs">
                <div class="nav-icon">❓</div>
                <div>FAQ's</div>
            </div>
            <!-- <div class="nav-box" data-content="case-study">
                <div class="nav-icon"><i class="fas fa-briefcase"></i></div>
                <div>Case Study</div>
            </div> -->
        </div>
    </div>

    </div>

    <div id="introduction" class="main-content active"
        style="padding: 20px; font-family: Arial, sans-serif; line-height: 1.6;">
    <h2 style="text-align: center; color: #333;">Introduction to Machine Learning</h2>
    <p style="margin-bottom: 15px; color: #555;">
        Welcome to the Machine Learning course! In this course, we will guide you through the essential concepts of machine learning, providing you with the foundational knowledge needed to succeed in this rapidly evolving field.
    </p>
    <p style="margin-bottom: 15px; color: #555;">
        Understanding machine learning is crucial for anyone looking to excel in data science and artificial intelligence. This course is designed to help you grasp core principles and best practices, equipping you with the skills to tackle real-world problems using machine learning techniques.
    </p>
    <p style="margin-bottom: 15px; color: #555;">
        Beyond just learning algorithms, this course will delve into the thought processes behind machine learning. We will explore key questions to ask during the model-building phase and how to evaluate different approaches effectively. This focus on the machine learning process distinguishes this course from others and is vital for succeeding in interviews and advancing your career in tech.
    </p>
    <div style="text-align: center; margin: 20px 0;">
        <img src="machinelearn_poster.jpg" alt="Machine Learning" style="max-width: 100%; height: auto; border-radius: 8px;">
    </div>
    <p style="margin-bottom: 15px; color: #555;">
        This additional content will enhance your learning experience, providing deeper insights and practical knowledge. By integrating real-world examples and advanced techniques, this course aims to transform you into a proficient machine learning practitioner capable of solving complex problems efficiently.
    </p>

    <!-- Read More button and additional content -->
    <div id="readMoreSection" style="text-align: center; margin-top: 20px;">
        <a href="#" onclick="toggleReadMore()"
            style="display: inline-block; padding: 12px 24px; font-size: 16px; color: #1a5928; background-color: #fff; text-decoration: none; border: 2px solid #1a5928; border-radius: 30px; transition: background-color 0.3s, color 0.3s;">
            <i class="fa fa-plus-circle" style="margin-right: 8px;"></i> Read More
        </a>
        <div id="moreContent" style="display: none; margin-top: 20px; color: #555;">
            <p>
                Understanding machine learning involves mastering both theoretical concepts and practical skills. This course covers various algorithms such as linear regression, decision trees, and neural networks, as well as their applications in real-world scenarios.
            </p>
            <p>
                We will dive deep into the principles of data preprocessing, feature engineering, and model evaluation. The course includes case studies that illustrate how machine learning principles are applied across industries. Additionally, you'll gain hands-on experience with tools and frameworks commonly used in the machine learning landscape.
            </p>
            <p>
                By engaging with this course, you will be equipped to build robust machine learning models and analyze their performance. You'll also have the ability to critique existing models, providing valuable insights into their strengths and weaknesses. This comprehensive approach ensures you are well-prepared for both interviews and practical applications in your career.
            </p>
        </div>
    </div>

    <!-- Buy Now button -->
    <div style="text-align: center; margin-top: 20px;">
        <a href="#" onclick="showPremiumOverlay()"
            style="display: inline-block; padding: 12px 24px; font-size: 16px; color: #fff; background-color: #1a5928; text-decoration: none; border-radius: 30px; transition: background-color 0.3s;">
            Buy Now
        </a>
    </div>
</div>


    <!-- Premium Access Overlay -->
    <div class="premium-overlay" id="premiumOverlay" style="display: none;">
        <div class="premium-content">
            <h2 class="premium-title">Unlock Premium Access</h2>
            <p class="premium-price">$199</p>
            <ul class="premium-features">
                <li><i class="fa fa-check-circle"></i> Full access to all course modules</li>
                <li><i class="fa fa-briefcase"></i> Exclusive case studies and projects</li>
                <li><i class="fa fa-user-tie"></i> 1-on-1 mentoring sessions</li>
                <li><i class="fa fa-certificate"></i> Certificate of completion</li>
                <li><i class="fa fa-refresh"></i> Lifetime updates</li>
            </ul>
            <button class="cta-button">Enroll Now</button>
            <button class="close-button" id="closeOverlay">Close</button>
        </div>
    </div>


    <!-- <div class="premium-overlay" id="premiumOverlay" style="display: none;">
            <div class="premium-content">
                <h2 class="premium-title">Unlock Premium Access</h2>
                <p class="premium-price">$199</p>
                <ul class="premium-features">
                    <li><i class="fa fa-check-circle"></i> Full access to all course modules</li>
                    <li><i class="fa fa-briefcase"></i> Exclusive case studies and projects</li>
                    <li><i class="fa fa-user-tie"></i> 1-on-1 mentoring sessions</li>
                    <li><i class="fa fa-certificate"></i> Certificate of completion</li>
                    <li><i class="fa fa-refresh"></i> Lifetime updates</li>
                </ul>
                <button class="cta-button">Enroll Now</button>
                <button class="close-button" id="closeOverlay">Close</button>
            </div> -->
    </div>


    </div>


    <div id="chapters" class="main-content">
        <h2>Machine Learning Course </h2><br>
        <ul class="chapter-list">
            <li class="chapter-item">
                <div class="chapter-header">
                    <span>Introduction to Machine Learning</span>
                    <span class="chapter-toggle">▼</span>
                </div>
                <div class="chapter-content">
                    <ul>
                        <li>What is machine learning and how does it differ from traditional programming?</li>
                        <li>Discuss the different types of machine learning: supervised, unsupervised, and reinforcement learning.</li>
                    </ul>
                </div>
            </li>
            <li class="chapter-item">
                <div class="chapter-header">
                    <span>Data Preprocessing</span>
                    <span class="chapter-toggle">▼</span>
                </div>
                <div class="chapter-content">
                    <ul>
                        <li>What are the key steps in preparing data for machine learning?</li>
                        <li>How do you handle missing values and categorical data in datasets?</li>
                    </ul>
                </div>
            </li>
            <li class="chapter-item">
                <div class="chapter-header">
                    <span>Supervised Learning Algorithms</span>
                    <span class="chapter-toggle">▼</span>
                </div>
                <div class="chapter-content">
                    <ul>
                        <li>What are the core concepts of regression and classification?</li>
                        <li>Explain common algorithms such as linear regression, decision trees, and support vector machines.</li>
                    </ul>
                </div>
            </li>
            <li class="chapter-item">
                <div class="chapter-header">
                    <span>Unsupervised Learning Techniques</span>
                    <span class="chapter-toggle">▼</span>
                </div>
                <div class="chapter-content">
                    <ul>
                        <li>What is clustering and how is it applied in machine learning?</li>
                        <li>Discuss dimensionality reduction techniques like PCA and t-SNE.</li>
                    </ul>
                </div>
            </li>
            <li class="chapter-item">
                <div class="chapter-header">
                    <span>Model Evaluation and Selection</span>
                    <span class="chapter-toggle">▼</span>
                </div>
                <div class="chapter-content">
                    <ul>
                        <li>What metrics are used to evaluate model performance in machine learning?</li>
                        <li>How do you perform cross-validation and why is it important?</li>
                    </ul>
                </div>
            </li>
            <li class="chapter-item">
                <div class="chapter-header">
                    <span>Feature Engineering</span>
                    <span class="chapter-toggle">▼</span>
                </div>
                <div class="chapter-content">
                    <ul>
                        <li>What is feature engineering and how does it impact model performance?</li>
                        <li>Discuss techniques for selecting and creating effective features.</li>
                    </ul>
                </div>
            </li>
            <li class="chapter-item">
                <div class="chapter-header">
                    <span>Deep Learning Fundamentals</span>
                    <span class="chapter-toggle">▼</span>
                </div>
                <div class="chapter-content">
                    <ul>
                        <li>What are neural networks and how do they differ from traditional algorithms?</li>
                        <li>Explain the architecture of deep learning models.</li>
                    </ul>
                </div>
            </li>
            <li class="chapter-item">
                <div class="chapter-header">
                    <span>Deployment and Productionization of Models</span>
                    <span class="chapter-toggle">▼</span>
                </div>
                <div class="chapter-content">
                    <ul>
                        <li>What are the best practices for deploying machine learning models into production?</li>
                        <li>Discuss tools and platforms commonly used for model deployment.</li>
                    </ul>
                </div>
            </li>
        </ul>
    </div>
    
    <!-- 
        <style>
            .main-content {
                padding: 20px;
                font-family: Arial, sans-serif;
            }
            .chapter-list {
                list-style-type: none;
                padding: 0;
            }
            .chapter-item {
                margin-bottom: 15px;
            }
            .chapter-header {
                display: flex;
                justify-content: space-between;
                cursor: pointer;
                padding: 10px;
                background-color: #f1f1f1;
                border-radius: 5px;
            }
            .chapter-content {
                padding: 10px;
                background-color: #f9f9f9;
                border-radius: 5px;
                display: none;
            }
            .solution {
                background-color: #1e1e1e;
                color: #d4d4d4;
                padding: 10px;
                border-radius: 5px;
                font-family: Consolas, "Courier New", monospace;
                display: none;
            }
        </style> -->
    </head>



    <body>





        <div id="case-study" class="main-content">
            <!-- buttons 10 topics case studies -->
            <div class="card-title">Click on below topic to Start the Case Study:</div>
            <div class="contentone-wrapperrr">
                <div class="content-header">
                    <!-- Case Study Topics -->
                    <!-- <span class="close-icon" onclick="redirectToPage()"><i class="fas fa-times"></i></span> -->
                </div>
                <div class="grid-layout">
                    <!-- Topics content -->
                    <div class="item-card">
                        <div class="item-card" data-target="data-expression">
                            <div class="card-icon"><i class="fas fa-database"></i></div>
                            <div class="card-title">Supervised Learning</div>
                        </div>
                    </div>
                    <div class="item-card">
                        <div class="item-card" onclick="scrollToSection()">
                            <div class="card-icon"><i class="fas fa-project-diagram"></i></div>
                            <div class="card-title">Unsupervised Learning</div>
                        </div>
                    </div>
                    <div class="item-card">
                        <div class="item-card" onclick="scrollToList()">
                            <div class="card-icon"><i class="fas fa-stream"></i></div>
                            <div class="card-title">Reinforcement Learning</div>
                        </div>
                    </div>
                    <div class="item-card">
                        <div class="item-card" onclick="scrollToFunctions()">
                            <div class="card-icon"><i class="fas fa-code"></i></div>
                            <div class="card-title">Deep Learning</div>
                        </div>
                    </div>
                    <div class="item-card">
                        <div class="item-card" onclick="scrollToObjects()">
                            <div class="card-icon"><i class="fas fa-cube"></i></div>
                            <div class="card-title">Natural Language Processing (NLP)</div>
                        </div>
                    </div>
                    <div class="item-card">
                        <div class="item-card" onclick="scrollToModularDesign()">
                            <div class="card-icon"><i class="fas fa-th-large"></i></div>
                            <div class="card-title">Feature Engineering</div>
                        </div>
                    </div>
                    <div class="item-card">
                        <div class="item-card" onclick="scrollToTextFiles()">
                            <div class="card-icon"><i class="fas fa-file-alt"></i></div>
                            <div class="card-title">Model Evaluation and Validation</div>
                        </div>
                    </div>
                    <div class="item-card">
                        <div class="item-card" onclick="scrollToDictionaries()">
                            <div class="card-icon"><i class="fas fa-key"></i></div>
                            <div class="card-title">Ensemble Learning

                            </div>
                        </div>
                    </div>
                    <div class="item-card">
                        <div class="item-card" onclick="scrollToOOP()">
                            <div class="card-icon"><i class="fas fa-object-group"></i></div>
                            <div class="card-title">Dimensionality Reduction</div>
                        </div>
                    </div>
                    <div class="item-card">
                        <div class="item-card" onclick="scrollToRecursion()">
                            <div class="card-icon"><i class="fas fa-sync-alt"></i></div>
                            <div class="card-title">Time Series Analysis and Forecasting</div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- buttons 10 topics case studies -->
            <br><br><br>
            <h2 id="machine-learning">Supervised Learning</h2><br>
<ul class="chapter-list">
    <li class="chapter-item">
        <div class="chapter-header">
            <span>Case Study 1: Supervised Learning</span>
            <span class="chapter-toggle">▼</span>
        </div>
        <div class="chapter-content">
            <p><strong>Objective:</strong> Understand the fundamentals of supervised learning and implement basic supervised learning algorithms.</p>
            <p><strong>Scenario:</strong> You are provided with a labeled dataset containing features and corresponding target values. Develop a Python script to train a supervised learning model (e.g., Linear Regression for regression tasks or Logistic Regression for classification tasks), make predictions, and evaluate the model's performance.</p>
            <p><strong>Key Concepts:</strong> Supervised Learning, Regression vs. Classification, Model Training, Prediction, Evaluation Metrics (e.g., Mean Squared Error, Accuracy, Precision, Recall).</p>
            <button onclick="toggleSolution(this)">Show Solution</button>
            <pre class="solution">
    import pandas as pd
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LinearRegression, LogisticRegression
    from sklearn.metrics import mean_squared_error, accuracy_score, classification_report

    # Loading the dataset
    data = pd.read_csv('labeled_data.csv')

    # Inspecting the first few rows
    print(data.head())

    # Defining features and target
    X = data.drop('target', axis=1)
    y = data['target']

    # Determining if it's a regression or classification task
    # For demonstration, let's assume it's a classification task
    # If regression, use LinearRegression instead of LogisticRegression

    # Splitting the dataset into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Initializing the Logistic Regression model
    model = LogisticRegression(max_iter=1000)

    # Training the model
    model.fit(X_train, y_train)

    # Making predictions on the test set
    y_pred = model.predict(X_test)

    # Evaluating the model
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Accuracy: {accuracy}")

    print("Classification Report:")
    print(classification_report(y_test, y_pred))

    # For regression tasks, use the following evaluation metrics
    # model = LinearRegression()
    # model.fit(X_train, y_train)
    # y_pred = model.predict(X_test)
    # mse = mean_squared_error(y_test, y_pred)
    # print(f"Mean Squared Error: {mse}")
            </pre>
        </div>
    </li>

    <li class="chapter-item">
        <div class="chapter-header">
            <span>Case Study 2: Exploratory Data Analysis (EDA)</span>
            <span class="chapter-toggle">▼</span>
        </div>
        <div class="chapter-content">
            <p><strong>Objective:</strong> Perform exploratory data analysis to understand the underlying patterns in the data.</p>
            <p><strong>Scenario:</strong> Given a dataset, create visualizations and summary statistics to uncover insights and relationships between variables.</p>
            <p><strong>Key Concepts:</strong> Data visualization, correlation analysis, summary statistics, pattern recognition.</p>
            <button onclick="toggleSolution(this)">Show Solution</button>
            <pre class="solution">
    import pandas as pd
    import matplotlib.pyplot as plt
    import seaborn as sns

    # Loading the dataset
    data = pd.read_csv('data.csv')

    # Summary statistics
    print(data.describe())

    # Correlation matrix
    corr = data.corr()
    plt.figure(figsize=(10,8))
    sns.heatmap(corr, annot=True, cmap='coolwarm')
    plt.title('Correlation Matrix')
    plt.show()

    # Distribution of a specific feature
    sns.histplot(data['feature_name'], kde=True)
    plt.title('Distribution of Feature Name')
    plt.show()

    # Scatter plot between two variables
    sns.scatterplot(x='feature1', y='feature2', data=data)
    plt.title('Feature1 vs Feature2')
    plt.show()
            </pre>
        </div>
    </li>

    <li class="chapter-item">
        <div class="chapter-header">
            <span>Case Study 3: Feature Engineering</span>
            <span class="chapter-toggle">▼</span>
        </div>
        <div class="chapter-content">
            <p><strong>Objective:</strong> Create new features to improve model performance.</p>
            <p><strong>Scenario:</strong> Enhance a dataset by generating new features that could provide better predictive power for a machine learning model.</p>
            <p><strong>Key Concepts:</strong> Creating interaction features, polynomial features, feature selection, dimensionality reduction.</p>
            <button onclick="toggleSolution(this)">Show Solution</button>
            <pre class="solution">
    import pandas as pd
    from sklearn.preprocessing import PolynomialFeatures
    from sklearn.feature_selection import SelectKBest, f_regression

    # Loading the dataset
    data = pd.read_csv('data.csv')

    # Creating interaction features
    data['feature1_feature2'] = data['feature1'] * data['feature2']

    # Generating polynomial features
    poly = PolynomialFeatures(degree=2, include_bias=False)
    poly_features = poly.fit_transform(data[['feature3', 'feature4']])
    poly_df = pd.DataFrame(poly_features, columns=poly.get_feature_names_out(['feature3', 'feature4']))
    data = pd.concat([data, poly_df], axis=1)

    # Feature selection
    X = data.drop('target', axis=1)
    y = data['target']
    selector = SelectKBest(score_func=f_regression, k=5)
    X_new = selector.fit_transform(X, y)
    selected_features = X.columns[selector.get_support()]
    print("Selected Features:", selected_features)

    # Final dataset with selected features
    data = data[selected_features.tolist() + ['target']]
    print(data.head())
            </pre>
        </div>
    </li>

    <li class="chapter-item">
        <div class="chapter-header">
            <span>Case Study 4: Training a Linear Regression Model</span>
            <span class="chapter-toggle">▼</span>
        </div>
        <div class="chapter-content">
            <p><strong>Objective:</strong> Implement and train a linear regression model to predict a continuous target variable.</p>
            <p><strong>Scenario:</strong> Using a preprocessed dataset, train a linear regression model and evaluate its performance.</p>
            <p><strong>Key Concepts:</strong> Linear regression, training and testing split, model evaluation metrics.</p>
            <button onclick="toggleSolution(this)">Show Solution</button>
            <pre class="solution">
    import pandas as pd
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LinearRegression
    from sklearn.metrics import mean_squared_error, r2_score

    # Loading the preprocessed dataset
    data = pd.read_csv('preprocessed_data.csv')

    # Splitting features and target
    X = data.drop('target', axis=1)
    y = data['target']

    # Splitting into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Initializing and training the model
    model = LinearRegression()
    model.fit(X_train, y_train)

    # Making predictions
    y_pred = model.predict(X_test)

    # Evaluating the model
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    print(f"Mean Squared Error: {mse}")
    print(f"R² Score: {r2}")
            </pre>
        </div>
    </li>

    <li class="chapter-item">
        <div class="chapter-header">
            <span>Case Study 5: Evaluating Model Performance</span>
            <span class="chapter-toggle">▼</span>
        </div>
        <div class="chapter-content">
            <p><strong>Objective:</strong> Assess and interpret the performance of machine learning models.</p>
            <p><strong>Scenario:</strong> After training multiple models, compare their performance using various evaluation metrics and choose the best-performing model.</p>
            <p><strong>Key Concepts:</strong> Evaluation metrics (accuracy, precision, recall, F1-score, ROC-AUC), cross-validation.</p>
            <button onclick="toggleSolution(this)">Show Solution</button>
            <pre class="solution">
    import pandas as pd
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.linear_model import LogisticRegression
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.metrics import classification_report, roc_auc_score

    # Loading the dataset
    data = pd.read_csv('classification_data.csv')

    # Splitting features and target
    X = data.drop('target', axis=1)
    y = data['target']

    # Splitting into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

    # Initializing models
    log_reg = LogisticRegression()
    rf_clf = RandomForestClassifier()

    # Training models
    log_reg.fit(X_train, y_train)
    rf_clf.fit(X_train, y_train)

    # Making predictions
    y_pred_log = log_reg.predict(X_test)
    y_pred_rf = rf_clf.predict(X_test)

    # Evaluating Logistic Regression
    print("Logistic Regression Classification Report:")
    print(classification_report(y_test, y_pred_log))
    print("ROC-AUC Score:", roc_auc_score(y_test, log_reg.predict_proba(X_test)[:,1]))

    # Evaluating Random Forest
    print("Random Forest Classification Report:")
    print(classification_report(y_test, y_pred_rf))
    print("ROC-AUC Score:", roc_auc_score(y_test, rf_clf.predict_proba(X_test)[:,1]))

    # Cross-validation scores
    cv_scores = cross_val_score(rf_clf, X, y, cv=5, scoring='roc_auc')
    print("Random Forest Cross-Validated ROC-AUC Scores:", cv_scores)
    print("Average ROC-AUC Score:", cv_scores.mean())
            </pre>
        </div>
    </li>

    <li class="chapter-item">
        <div class="chapter-header">
            <span>Case Study 6: Classification with Logistic Regression</span>
            <span class="chapter-toggle">▼</span>
        </div>
        <div class="chapter-content">
            <p><strong>Objective:</strong> Implement a logistic regression model for binary classification tasks.</p>
            <p><strong>Scenario:</strong> Using a dataset with a binary target variable, train a logistic regression model and evaluate its performance.</p>
            <p><strong>Key Concepts:</strong> Logistic regression, binary classification, probability thresholds, confusion matrix.</p>
            <button onclick="toggleSolution(this)">Show Solution</button>
            <pre class="solution">
    import pandas as pd
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score

    # Loading the dataset
    data = pd.read_csv('binary_classification_data.csv')

    # Splitting features and target
    X = data.drop('target', axis=1)
    y = data['target']

    # Splitting into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)

    # Initializing and training the model
    model = LogisticRegression()
    model.fit(X_train, y_train)

    # Making predictions
    y_pred = model.predict(X_test)

    # Evaluating the model
    conf_matrix = confusion_matrix(y_test, y_pred)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)

    print("Confusion Matrix:")
    print(conf_matrix)
    print(f"Accuracy: {accuracy}")
    print(f"Precision: {precision}")
    print(f"Recall: {recall}")
            </pre>
        </div>
    </li>

    <li class="chapter-item">
        <div class="chapter-header">
            <span>Case Study 7: Decision Trees and Random Forests</span>
            <span class="chapter-toggle">▼</span>
        </div>
        <div class="chapter-content">
            <p><strong>Objective:</strong> Utilize decision trees and random forests for classification tasks.</p>
            <p><strong>Scenario:</strong> Train both a decision tree and a random forest classifier on a dataset, compare their performances, and interpret the results.</p>
            <p><strong>Key Concepts:</strong> Decision trees, random forests, overfitting, feature importance.</p>
            <button onclick="toggleSolution(this)">Show Solution</button>
            <pre class="solution">
    import pandas as pd
    from sklearn.model_selection import train_test_split
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.metrics import accuracy_score, classification_report

    # Loading the dataset
    data = pd.read_csv('classification_data.csv')

    # Splitting features and target
    X = data.drop('target', axis=1)
    y = data['target']

    # Splitting into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

    # Initializing models
    dt_clf = DecisionTreeClassifier(random_state=42)
    rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)

    # Training models
    dt_clf.fit(X_train, y_train)
    rf_clf.fit(X_train, y_train)

    # Making predictions
    y_pred_dt = dt_clf.predict(X_test)
    y_pred_rf = rf_clf.predict(X_test)

    # Evaluating Decision Tree
    print("Decision Tree Accuracy:", accuracy_score(y_test, y_pred_dt))
    print("Decision Tree Classification Report:")
    print(classification_report(y_test, y_pred_dt))

    # Evaluating Random Forest
    print("Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))
    print("Random Forest Classification Report:")
    print(classification_report(y_test, y_pred_rf))

    # Feature Importance from Random Forest
    importances = rf_clf.feature_importances_
    feature_names = X.columns
    feature_importance = pd.Series(importances, index=feature_names).sort_values(ascending=False)
    print("Feature Importances:")
    print(feature_importance)
            </pre>
        </div>
    </li>

    <li class="chapter-item">
        <div class="chapter-header">
            <span>Case Study 8: Support Vector Machines (SVM)</span>
            <span class="chapter-toggle">▼</span>
        </div>
        <div class="chapter-content">
            <p><strong>Objective:</strong> Apply Support Vector Machines for classification tasks.</p>
            <p><strong>Scenario:</strong> Train an SVM classifier on a dataset, tune its hyperparameters, and evaluate its performance.</p>
            <p><strong>Key Concepts:</strong> Support Vector Machines, kernel functions, hyperparameter tuning, margin optimization.</p>
            <button onclick="toggleSolution(this)">Show Solution</button>
            <pre class="solution">
    import pandas as pd
    from sklearn.model_selection import train_test_split, GridSearchCV
    from sklearn.svm import SVC
    from sklearn.metrics import classification_report, accuracy_score

    # Loading the dataset
    data = pd.read_csv('svm_classification_data.csv')

    # Splitting features and target
    X = data.drop('target', axis=1)
    y = data['target']

    # Splitting into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Initializing SVM
    svm = SVC()

    # Hyperparameter tuning using GridSearchCV
    param_grid = {
        'C': [0.1, 1, 10],
        'kernel': ['linear', 'rbf'],
        'gamma': ['scale', 'auto']
    }
    grid = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy')
    grid.fit(X_train, y_train)

    # Best parameters and estimator
    print("Best Parameters:", grid.best_params_)
    best_svm = grid.best_estimator_

    # Making predictions
    y_pred = best_svm.predict(X_test)

    # Evaluating the model
    print("SVM Accuracy:", accuracy_score(y_test, y_pred))
    print("Classification Report:")
    print(classification_report(y_test, y_pred))
            </pre>
        </div>
    </li>

    <li class="chapter-item">
        <div class="chapter-header">
            <span>Case Study 9: Clustering with K-Means</span>
            <span class="chapter-toggle">▼</span>
        </div>
        <div class="chapter-content">
            <p><strong>Objective:</strong> Implement K-Means clustering to identify patterns in unlabeled data.</p>
            <p><strong>Scenario:</strong> Apply K-Means clustering on a dataset to segment the data into distinct groups and visualize the clusters.</p>
            <p><strong>Key Concepts:</strong> K-Means clustering, elbow method, cluster visualization, silhouette score.</p>
            <button onclick="toggleSolution(this)">Show Solution</button>
            <pre class="solution">
    import pandas as pd
    import matplotlib.pyplot as plt
    from sklearn.cluster import KMeans
    from sklearn.metrics import silhouette_score

    # Loading the dataset
    data = pd.read_csv('clustering_data.csv')

    # Selecting features for clustering
    X = data[['feature1', 'feature2']]

    # Determining the optimal number of clusters using the elbow method
    wcss = []
    for i in range(1, 11):
        kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
        kmeans.fit(X)
        wcss.append(kmeans.inertia_)
    plt.plot(range(1, 11), wcss)
    plt.title('Elbow Method')
    plt.xlabel('Number of clusters')
    plt.ylabel('WCSS')
    plt.show()

    # From the elbow plot, assume the optimal number of clusters is 3
    kmeans = KMeans(n_clusters=3, random_state=42)
    y_kmeans = kmeans.fit_predict(X)

    # Adding cluster labels to the original data
    data['Cluster'] = y_kmeans

    # Calculating silhouette score
    score = silhouette_score(X, y_kmeans)
    print(f"Silhouette Score: {score}")

    # Visualizing the clusters
    plt.scatter(X['feature1'], X['feature2'], c=y_kmeans, cmap='viridis')
    plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], s=300, c='red', label='Centroids')
    plt.title('K-Means Clustering')
    plt.xlabel('Feature1')
    plt.ylabel('Feature2')
    plt.legend()
    plt.show()
            </pre>
        </div>
    </li>

    <li class="chapter-item">
        <div class="chapter-header">
            <span>Case Study 10: Neural Networks with TensorFlow</span>
            <span class="chapter-toggle">▼</span>
        </div>
        <div class="chapter-content">
            <p><strong>Objective:</strong> Build and train a neural network for image classification using TensorFlow.</p>
            <p><strong>Scenario:</strong> Develop a convolutional neural network (CNN) to classify images from the CIFAR-10 dataset.</p>
            <p><strong>Key Concepts:</strong> Neural networks, convolutional layers, activation functions, model compilation, training and evaluation.</p>
            <button onclick="toggleSolution(this)">Show Solution</button>
            <pre class="solution">
    import tensorflow as tf
    from tensorflow.keras import datasets, layers, models
    import matplotlib.pyplot as plt

    # Loading the CIFAR-10 dataset
    (train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()

    # Normalizing pixel values
    train_images, test_images = train_images / 255.0, test_images / 255.0

    # Defining class names
    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
                   'dog', 'frog', 'horse', 'ship', 'truck']

    # Building the CNN model
    model = models.Sequential([
        layers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),
        layers.MaxPooling2D((2,2)),
        layers.Conv2D(64, (3,3), activation='relu'),
        layers.MaxPooling2D((2,2)),
        layers.Conv2D(64, (3,3), activation='relu'),
        layers.Flatten(),
        layers.Dense(64, activation='relu'),
        layers.Dense(10)
    ])

    # Compiling the model
    model.compile(optimizer='adam',
                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                  metrics=['accuracy'])

    # Training the model
    history = model.fit(train_images, train_labels, epochs=10,
                        validation_data=(test_images, test_labels))

    # Evaluating the model
    plt.plot(history.history['accuracy'], label='accuracy')
    plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.ylim([0, 1])
    plt.legend(loc='lower right')
    plt.show()

    test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)
    print(f"Test Accuracy: {test_acc}")
            </pre>
        </div>
    </li>
</ul>


            <!--Unsupervised Learning -->

            <div id="case-study2" class="main-content2">
                <h2 id="unsupervised-learning">Unsupervised Learning</h2><br>
                <ul class="chapter-list">
                    <li class="chapter-item">
                        <div class="chapter-header">
                            <span>Case Study 1: Clustering with K-Means</span>
                            <span class="chapter-toggle">▼</span>
                        </div>
                        <div class="chapter-content">
                            <p><strong>Objective:</strong> Learn how to perform K-Means clustering to identify natural groupings in data.</p>
                            <p><strong>Scenario:</strong> Given a dataset containing customer information, apply K-Means clustering to segment customers into distinct groups based on their purchasing behavior.</p>
                            <p><strong>Key Concepts:</strong> K-Means algorithm, cluster initialization, inertia, elbow method, cluster visualization.</p>
                            <button onclick="toggleSolution(this)">Show Solution</button>
                            <pre class="solution">
                    import pandas as pd
                    import matplotlib.pyplot as plt
                    from sklearn.cluster import KMeans
                    from sklearn.preprocessing import StandardScaler
            
                    # Loading the dataset
                    data = pd.read_csv('customer_data.csv')
            
                    # Selecting relevant features
                    features = ['Annual Income (k$)', 'Spending Score (1-100)']
                    X = data[features]
            
                    # Feature scaling
                    scaler = StandardScaler()
                    X_scaled = scaler.fit_transform(X)
            
                    # Determining the optimal number of clusters using the elbow method
                    wcss = []
                    for i in range(1, 11):
                        kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
                        kmeans.fit(X_scaled)
                        wcss.append(kmeans.inertia_)
            
                    # Plotting the elbow graph
                    plt.plot(range(1, 11), wcss, marker='o')
                    plt.title('Elbow Method')
                    plt.xlabel('Number of clusters')
                    plt.ylabel('WCSS')
                    plt.show()
            
                    # From the elbow plot, assume the optimal number of clusters is 5
                    kmeans = KMeans(n_clusters=5, init='k-means++', random_state=42)
                    y_kmeans = kmeans.fit_predict(X_scaled)
            
                    # Adding cluster labels to the original data
                    data['Cluster'] = y_kmeans
            
                    # Visualizing the clusters
                    plt.scatter(data['Annual Income (k$)'], data['Spending Score (1-100)'], c=data['Cluster'], cmap='viridis')
                    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red', label='Centroids')
                    plt.title('Customer Segments')
                    plt.xlabel('Annual Income (k$)')
                    plt.ylabel('Spending Score (1-100)')
                    plt.legend()
                    plt.show()
            
                    # Displaying cluster centers
                    print("Cluster Centers:")
                    print(kmeans.cluster_centers_)
                            </pre>
                        </div>
                    </li>
            
                    <li class="chapter-item">
                        <div class="chapter-header">
                            <span>Case Study 2: Hierarchical Clustering</span>
                            <span class="chapter-toggle">▼</span>
                        </div>
                        <div class="chapter-content">
                            <p><strong>Objective:</strong> Understand and implement hierarchical clustering to create a dendrogram and identify clusters.</p>
                            <p><strong>Scenario:</strong> Using the same customer dataset, perform hierarchical clustering to visualize the dendrogram and determine the optimal number of clusters.</p>
                            <p><strong>Key Concepts:</strong> Agglomerative clustering, dendrogram, linkage methods, hierarchical structure.</p>
                            <button onclick="toggleSolution(this)">Show Solution</button>
                            <pre class="solution">
                    import pandas as pd
                    import matplotlib.pyplot as plt
                    from sklearn.preprocessing import StandardScaler
                    from scipy.cluster.hierarchy import dendrogram, linkage
                    from sklearn.cluster import AgglomerativeClustering
            
                    # Loading the dataset
                    data = pd.read_csv('customer_data.csv')
            
                    # Selecting relevant features
                    features = ['Annual Income (k$)', 'Spending Score (1-100)']
                    X = data[features]
            
                    # Feature scaling
                    scaler = StandardScaler()
                    X_scaled = scaler.fit_transform(X)
            
                    # Creating the linkage matrix
                    Z = linkage(X_scaled, method='ward')
            
                    # Plotting the dendrogram
                    plt.figure(figsize=(10, 7))
                    dendrogram(Z, truncate_mode='lastp', p=12, leaf_rotation=45., leaf_font_size=12., show_contracted=True)
                    plt.title('Dendrogram for Hierarchical Clustering')
                    plt.xlabel('Cluster Size')
                    plt.ylabel('Distance')
                    plt.show()
            
                    # Applying Agglomerative Clustering
                    hc = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')
                    y_hc = hc.fit_predict(X_scaled)
            
                    # Adding cluster labels to the original data
                    data['Cluster'] = y_hc
            
                    # Visualizing the clusters
                    plt.scatter(data['Annual Income (k$)'], data['Spending Score (1-100)'], c=data['Cluster'], cmap='rainbow')
                    plt.title('Customer Segments (Hierarchical Clustering)')
                    plt.xlabel('Annual Income (k$)')
                    plt.ylabel('Spending Score (1-100)')
                    plt.show()
                            </pre>
                        </div>
                    </li>
            
                    <li class="chapter-item">
                        <div class="chapter-header">
                            <span>Case Study 3: Principal Component Analysis (PCA)</span>
                            <span class="chapter-toggle">▼</span>
                        </div>
                        <div class="chapter-content">
                            <p><strong>Objective:</strong> Reduce the dimensionality of a dataset while retaining most of the variance using PCA.</p>
                            <p><strong>Scenario:</strong> Apply PCA on a high-dimensional dataset to visualize the data in 2D space and interpret the principal components.</p>
                            <p><strong>Key Concepts:</strong> Dimensionality reduction, eigenvectors, eigenvalues, explained variance.</p>
                            <button onclick="toggleSolution(this)">Show Solution</button>
                            <pre class="solution">
                    import pandas as pd
                    import matplotlib.pyplot as plt
                    from sklearn.decomposition import PCA
                    from sklearn.preprocessing import StandardScaler
            
                    # Loading the dataset
                    data = pd.read_csv('high_dimensional_data.csv')
            
                    # Separating features
                    X = data.drop('target', axis=1)
            
                    # Feature scaling
                    scaler = StandardScaler()
                    X_scaled = scaler.fit_transform(X)
            
                    # Applying PCA
                    pca = PCA(n_components=2)
                    principal_components = pca.fit_transform(X_scaled)
                    pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])
            
                    # Adding target labels for visualization
                    pca_df['Target'] = data['target']
            
                    # Visualizing the PCA results
                    plt.figure(figsize=(8,6))
                    plt.scatter(pca_df['PC1'], pca_df['PC2'], c=pca_df['Target'], cmap='plasma')
                    plt.xlabel('Principal Component 1')
                    plt.ylabel('Principal Component 2')
                    plt.title('PCA of High-Dimensional Data')
                    plt.colorbar()
                    plt.show()
            
                    # Explained variance
                    print(f"Explained variance by PC1: {pca.explained_variance_ratio_[0]:.2f}")
                    print(f"Explained variance by PC2: {pca.explained_variance_ratio_[1]:.2f}")
                            </pre>
                        </div>
                    </li>
            
                    <li class="chapter-item">
                        <div class="chapter-header">
                            <span>Case Study 4: Association Rule Learning with Apriori</span>
                            <span class="chapter-toggle">▼</span>
                        </div>
                        <div class="chapter-content">
                            <p><strong>Objective:</strong> Discover interesting associations and relationships between variables in large datasets.</p>
                            <p><strong>Scenario:</strong> Analyze transaction data to find frequently co-occurring items using the Apriori algorithm.</p>
                            <p><strong>Key Concepts:</strong> Market basket analysis, support, confidence, lift, frequent itemsets.</p>
                            <button onclick="toggleSolution(this)">Show Solution</button>
                            <pre class="solution">
                    import pandas as pd
                    from mlxtend.frequent_patterns import apriori, association_rules
            
                    # Loading the dataset
                    data = pd.read_csv('transactions.csv')
            
                    # Preprocessing the data for Apriori
                    basket = (data.groupby(['Transaction', 'Item'])['Item']
                              .count().unstack().reset_index().fillna(0)
                              .set_index('Transaction'))
            
                    # Converting quantities to 1 and 0
                    basket = basket.applymap(lambda x: 1 if x > 0 else 0)
            
                    # Applying Apriori to find frequent itemsets
                    frequent_itemsets = apriori(basket, min_support=0.02, use_colnames=True)
            
                    # Generating association rules
                    rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1)
            
                    # Displaying the rules
                    print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])
                            </pre>
                        </div>
                    </li>
            
                    <li class="chapter-item">
                        <div class="chapter-header">
                            <span>Case Study 5: DBSCAN Clustering</span>
                            <span class="chapter-toggle">▼</span>
                        </div>
                        <div class="chapter-content">
                            <p><strong>Objective:</strong> Utilize DBSCAN to identify clusters of varying shapes and detect outliers.</p>
                            <p><strong>Scenario:</strong> Apply DBSCAN clustering on spatial data to find densely packed regions and identify noise points.</p>
                            <p><strong>Key Concepts:</strong> Density-based clustering, epsilon (ε), minimum samples, noise detection.</p>
                            <button onclick="toggleSolution(this)">Show Solution</button>
                            <pre class="solution">
                    import pandas as pd
                    import matplotlib.pyplot as plt
                    from sklearn.cluster import DBSCAN
                    from sklearn.preprocessing import StandardScaler
            
                    # Loading the dataset
                    data = pd.read_csv('spatial_data.csv')
            
                    # Selecting relevant features
                    X = data[['X_coordinate', 'Y_coordinate']]
            
                    # Feature scaling
                    scaler = StandardScaler()
                    X_scaled = scaler.fit_transform(X)
            
                    # Applying DBSCAN
                    dbscan = DBSCAN(eps=0.5, min_samples=5)
                    clusters = dbscan.fit_predict(X_scaled)
            
                    # Adding cluster labels to the data
                    data['Cluster'] = clusters
            
                    # Visualizing the clusters
                    plt.scatter(data['X_coordinate'], data['Y_coordinate'], c=data['Cluster'], cmap='Paired')
                    plt.title('DBSCAN Clustering')
                    plt.xlabel('X Coordinate')
                    plt.ylabel('Y Coordinate')
                    plt.show()
            
                    # Counting the number of points in each cluster
                    print(data['Cluster'].value_counts())
                            </pre>
                        </div>
                    </li>
            
                    <li class="chapter-item">
                        <div class="chapter-header">
                            <span>Case Study 6: Dimensionality Reduction with t-SNE</span>
                            <span class="chapter-toggle">▼</span>
                        </div>
                        <div class="chapter-content">
                            <p><strong>Objective:</strong> Visualize high-dimensional data in a lower-dimensional space using t-SNE.</p>
                            <p><strong>Scenario:</strong> Use t-SNE to project high-dimensional image data into 2D space for visualization and pattern recognition.</p>
                            <p><strong>Key Concepts:</strong> t-Distributed Stochastic Neighbor Embedding (t-SNE), perplexity, dimensionality reduction, visualization.</p>
                            <button onclick="toggleSolution(this)">Show Solution</button>
                            <pre class="solution">
                    import pandas as pd
                    import matplotlib.pyplot as plt
                    from sklearn.manifold import TSNE
                    from sklearn.preprocessing import StandardScaler
            
                    # Loading the dataset
                    data = pd.read_csv('high_dimensional_images.csv')
            
                    # Selecting features
                    X = data.drop('label', axis=1)
            
                    # Feature scaling
                    scaler = StandardScaler()
                    X_scaled = scaler.fit_transform(X)
            
                    # Applying t-SNE
                    tsne = TSNE(n_components=2, perplexity=30, n_iter=300)
                    X_tsne = tsne.fit_transform(X_scaled)
            
                    # Creating a DataFrame for visualization
                    tsne_df = pd.DataFrame(data=X_tsne, columns=['Dim1', 'Dim2'])
                    tsne_df['Label'] = data['label']
            
                    # Plotting the t-SNE results
                    plt.figure(figsize=(10,8))
                    scatter = plt.scatter(tsne_df['Dim1'], tsne_df['Dim2'], c=tsne_df['Label'], cmap='tab10')
                    plt.title('t-SNE Visualization of High-Dimensional Data')
                    plt.xlabel('Dimension 1')
                    plt.ylabel('Dimension 2')
                    plt.legend(*scatter.legend_elements(), title="Classes")
                    plt.show()
                            </pre>
                        </div>
                    </li>
            
                    <li class="chapter-item">
                        <div class="chapter-header">
                            <span>Case Study 7: Anomaly Detection with Isolation Forest</span>
                            <span class="chapter-toggle">▼</span>
                        </div>
                        <div class="chapter-content">
                            <p><strong>Objective:</strong> Detect outliers and anomalies in data using the Isolation Forest algorithm.</p>
                            <p><strong>Scenario:</strong> Implement an Isolation Forest to identify fraudulent transactions in a financial dataset.</p>
                            <p><strong>Key Concepts:</strong> Anomaly detection, isolation trees, contamination, outlier scoring.</p>
                            <button onclick="toggleSolution(this)">Show Solution</button>
                            <pre class="solution">
                    import pandas as pd
                    import matplotlib.pyplot as plt
                    from sklearn.ensemble import IsolationForest
                    from sklearn.preprocessing import StandardScaler
            
                    # Loading the dataset
                    data = pd.read_csv('financial_transactions.csv')
            
                    # Selecting features
                    X = data[['Amount', 'Transaction_Frequency']]
            
                    # Feature scaling
                    scaler = StandardScaler()
                    X_scaled = scaler.fit_transform(X)
            
                    # Initializing Isolation Forest
                    iso_forest = IsolationForest(contamination=0.01, random_state=42)
                    iso_forest.fit(X_scaled)
            
                    # Predicting anomalies
                    data['Anomaly'] = iso_forest.predict(X_scaled)
                    data['Anomaly'] = data['Anomaly'].map({1: 0, -1: 1})  # 1 for anomaly, 0 for normal
            
                    # Visualizing the anomalies
                    plt.scatter(data['Amount'], data['Transaction_Frequency'], c=data['Anomaly'], cmap='coolwarm')
                    plt.title('Anomaly Detection with Isolation Forest')
                    plt.xlabel('Amount')
                    plt.ylabel('Transaction Frequency')
                    plt.show()
            
                    # Displaying detected anomalies
                    anomalies = data[data['Anomaly'] == 1]
                    print("Detected Anomalies:")
                    print(anomalies)
                            </pre>
                        </div>
                    </li>
            
                    <li class="chapter-item">
                        <div class="chapter-header">
                            <span>Case Study 8: Feature Learning with Autoencoders</span>
                            <span class="chapter-toggle">▼</span>
                        </div>
                        <div class="chapter-content">
                            <p><strong>Objective:</strong> Learn how to use autoencoders for feature extraction and dimensionality reduction.</p>
                            <p><strong>Scenario:</strong> Build an autoencoder to compress and reconstruct image data, then use the encoded features for clustering.</p>
                            <p><strong>Key Concepts:</strong> Neural network autoencoders, encoder-decoder architecture, latent space, reconstruction loss.</p>
                            <button onclick="toggleSolution(this)">Show Solution</button>
                            <pre class="solution">
                    import numpy as np
                    import pandas as pd
                    import matplotlib.pyplot as plt
                    from tensorflow.keras.layers import Input, Dense
                    from tensorflow.keras.models import Model
                    from sklearn.cluster import KMeans
                    from sklearn.preprocessing import StandardScaler
            
                    # Loading the dataset
                    data = pd.read_csv('image_data.csv')
            
                    # Normalizing the data
                    scaler = StandardScaler()
                    X_scaled = scaler.fit_transform(data)
            
                    # Defining the autoencoder architecture
                    input_dim = X_scaled.shape[1]
                    encoding_dim = 32  # Size of the encoded representation
            
                    # Input layer
                    input_layer = Input(shape=(input_dim,))
            
                    # Encoder
                    encoded = Dense(encoding_dim, activation='relu')(input_layer)
            
                    # Decoder
                    decoded = Dense(input_dim, activation='sigmoid')(encoded)
            
                    # Autoencoder model
                    autoencoder = Model(inputs=input_layer, outputs=decoded)
            
                    # Encoder model
                    encoder = Model(inputs=input_layer, outputs=encoded)
            
                    # Compiling the autoencoder
                    autoencoder.compile(optimizer='adam', loss='mean_squared_error')
            
                    # Training the autoencoder
                    autoencoder.fit(X_scaled, X_scaled,
                                    epochs=50,
                                    batch_size=256,
                                    shuffle=True,
                                    validation_split=0.2)
            
                    # Obtaining the encoded features
                    encoded_features = encoder.predict(X_scaled)
            
                    # Applying K-Means clustering on the encoded features
                    kmeans = KMeans(n_clusters=5, random_state=42)
                    clusters = kmeans.fit_predict(encoded_features)
            
                    # Adding cluster labels to the data
                    data['Cluster'] = clusters
            
                    # Visualizing the clusters (using first two principal components for visualization)
                    from sklearn.decomposition import PCA
                    pca = PCA(n_components=2)
                    principal_components = pca.fit_transform(encoded_features)
                    pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])
                    pca_df['Cluster'] = clusters
            
                    plt.figure(figsize=(8,6))
                    scatter = plt.scatter(pca_df['PC1'], pca_df['PC2'], c=pca_df['Cluster'], cmap='Set1')
                    plt.title('Clustering on Encoded Features with Autoencoders')
                    plt.xlabel('Principal Component 1')
                    plt.ylabel('Principal Component 2')
                    plt.legend(*scatter.legend_elements(), title="Clusters")
                    plt.show()
                            </pre>
                        </div>
                    </li>
            
                    <li class="chapter-item">
                        <div class="chapter-header">
                            <span>Case Study 9: Gaussian Mixture Models (GMM)</span>
                            <span class="chapter-toggle">▼</span>
                        </div>
                        <div class="chapter-content">
                            <p><strong>Objective:</strong> Implement Gaussian Mixture Models for probabilistic clustering.</p>
                            <p><strong>Scenario:</strong> Use GMM to model the distribution of data points and assign probabilities to cluster memberships.</p>
                            <p><strong>Key Concepts:</strong> Probabilistic clustering, expectation-maximization (EM) algorithm, covariance types.</p>
                            <button onclick="toggleSolution(this)">Show Solution</button>
                            <pre class="solution">
                    import pandas as pd
                    import matplotlib.pyplot as plt
                    from sklearn.mixture import GaussianMixture
                    from sklearn.preprocessing import StandardScaler
            
                    # Loading the dataset
                    data = pd.read_csv('gmm_data.csv')
            
                    # Selecting relevant features
                    X = data[['Feature1', 'Feature2']]
            
                    # Feature scaling
                    scaler = StandardScaler()
                    X_scaled = scaler.fit_transform(X)
            
                    # Applying Gaussian Mixture Model
                    gmm = GaussianMixture(n_components=3, covariance_type='full', random_state=42)
                    gmm.fit(X_scaled)
                    clusters = gmm.predict(X_scaled)
                    probabilities = gmm.predict_proba(X_scaled)
            
                    # Adding cluster labels to the data
                    data['Cluster'] = clusters
            
                    # Visualizing the clusters
                    plt.scatter(data['Feature1'], data['Feature2'], c=data['Cluster'], cmap='viridis')
                    plt.title('Gaussian Mixture Model Clustering')
                    plt.xlabel('Feature1')
                    plt.ylabel('Feature2')
                    plt.show()
            
                    # Displaying cluster probabilities for the first 5 data points
                    print("Cluster Probabilities for first 5 data points:")
                    print(probabilities[:5])
                            </pre>
                        </div>
                    </li>
            
                    <li class="chapter-item">
                        <div class="chapter-header">
                            <span>Case Study 10: Self-Organizing Maps (SOM)</span>
                            <span class="chapter-toggle">▼</span>
                        </div>
                        <div class="chapter-content">
                            <p><strong>Objective:</strong> Utilize Self-Organizing Maps for visualizing high-dimensional data.</p>
                            <p><strong>Scenario:</strong> Apply SOM to cluster and visualize customer data, enabling pattern discovery and data segmentation.</p>
                            <p><strong>Key Concepts:</strong> Self-Organizing Maps, neural networks, grid-based clustering, visualization.</p>
                            <button onclick="toggleSolution(this)">Show Solution</button>
                            <pre class="solution">
                    import pandas as pd
                    import matplotlib.pyplot as plt
                    from minisom import MiniSom
                    from sklearn.preprocessing import StandardScaler
            
                    # Loading the dataset
                    data = pd.read_csv('customer_data.csv')
            
                    # Selecting relevant features
                    features = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']
                    X = data[features]
            
                    # Feature scaling
                    scaler = StandardScaler()
                    X_scaled = scaler.fit_transform(X)
            
                    # Initializing and training the Self-Organizing Map
                    som_size = 7
                    som = MiniSom(som_size, som_size, X_scaled.shape[1], sigma=1.0, learning_rate=0.5, random_seed=42)
                    som.train_random(X_scaled, 100)
            
                    # Getting the cluster assignments
                    win_map = som.win_map(X_scaled)
                    cluster_labels = [som.winner(x) for x in X_scaled]
                    data['SOM_Cluster'] = cluster_labels
            
                    # Visualizing the SOM
                    plt.figure(figsize=(10, 10))
                    for i, x in enumerate(X_scaled):
                        w = som.winner(x)
                        plt.text(w[0]+0.5, w[1]+0.5, str(data['SOM_Cluster'][i]),
                                 color=plt.cm.rainbow(data['SOM_Cluster'][i] / 10.),
                                 fontdict={'weight': 'bold', 'size': 9})
                    plt.title('Self-Organizing Map')
                    plt.xlim([0, som_size])
                    plt.ylim([0, som_size])
                    plt.show()
            
                    # Displaying the clusters
                    print("SOM Cluster Assignments:")
                    print(data['SOM_Cluster'].value_counts())
                            </pre>
                        </div>
                    </li>
                </ul>
            </div>
            

                <!-- lists -->

                <div id="case-study3" class="main-content3">
                    <h2 id="reinforcement-learning-section">Reinforcement Learning</h2><br>
                    <ul class="chapter-list">
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 1: Implementing a Basic Q-Learning Agent</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Understand the fundamentals of Q-Learning and implement a basic agent.</p>
                                <p><strong>Scenario:</strong> Create a Q-Learning agent to solve the FrozenLake environment from OpenAI Gym. Train the agent to navigate the frozen lake without falling into holes.</p>
                                <p><strong>Key Concepts:</strong> Q-Learning, exploration vs. exploitation, Q-table.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">import gym
                import numpy as np
                
                # Initialize the FrozenLake environment
                env = gym.make('FrozenLake-v1', is_slippery=False)
                
                # Initialize Q-table
                q_table = np.zeros([env.observation_space.n, env.action_space.n])
                
                # Set learning parameters
                alpha = 0.8
                gamma = 0.95
                epsilon = 0.1
                num_episodes = 2000
                
                # Q-Learning algorithm
                for episode in range(num_episodes):
                    state = env.reset()
                    done = False
                
                    while not done:
                        if np.random.uniform(0, 1) < epsilon:
                            action = env.action_space.sample()  # Explore
                        else:
                            action = np.argmax(q_table[state])  # Exploit
                
                        next_state, reward, done, _ = env.step(action)
                
                        # Update Q-table
                        q_table[state, action] = q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state]) - q_table[state, action])
                
                        state = next_state
                
                # Test the trained agent
                state = env.reset()
                done = False
                total_reward = 0
                
                while not done:
                    action = np.argmax(q_table[state])
                    state, reward, done, _ = env.step(action)
                    total_reward += reward
                    env.render()
                
                print("Total Reward:", total_reward)</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 2: Training an Agent in CartPole Environment</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Apply reinforcement learning to balance a pole on a cart.</p>
                                <p><strong>Scenario:</strong> Use the CartPole-v1 environment from OpenAI Gym to train an agent that can keep the pole balanced for as long as possible.</p>
                                <p><strong>Key Concepts:</strong> Policy iteration, reward structure, environment interaction.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">import gym
                import numpy as np
                
                env = gym.make('CartPole-v1')
                
                # Discretize the observation space
                state_space = [20] * len(env.observation_space.high)
                state_bounds = list(zip(env.observation_space.low, env.observation_space.high))
                state_bounds[1] = [-0.5, 0.5]
                state_bounds[3] = [-math.radians(50), math.radians(50)]
                q_table = np.zeros(state_space + [env.action_space.n])
                
                def discretize(state):
                    ratios = [(state[i] + abs(state_bounds[i][0])) / (state_bounds[i][1] - state_bounds[i][0]) for i in range(len(state))]
                    new_state = [int(round((state_space[i]-1) * ratios[i])) for i in range(len(state))]
                    new_state = [min(state_space[i]-1, max(0, new_state[i])) for i in range(len(state))]
                    return tuple(new_state)
                
                alpha = 0.1
                gamma = 0.99
                epsilon = 1.0
                epsilon_decay = 0.995
                num_episodes = 10000
                
                for episode in range(num_episodes):
                    state = discretize(env.reset())
                    done = False
                
                    while not done:
                        if np.random.random() < epsilon:
                            action = env.action_space.sample()
                        else:
                            action = np.argmax(q_table[state])
                
                        next_state, reward, done, _ = env.step(action)
                        next_state = discretize(next_state)
                
                        q_table[state][action] += alpha * (reward + gamma * np.max(q_table[next_state]) - q_table[state][action])
                
                        state = next_state
                
                    epsilon = max(epsilon * epsilon_decay, 0.01)
                
                # Testing the trained agent
                state = discretize(env.reset())
                done = False
                total_reward = 0
                
                while not done:
                    action = np.argmax(q_table[state])
                    state, reward, done, _ = env.step(action)
                    total_reward += reward
                    env.render()
                
                print("Total Reward:", total_reward)</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 3: Implementing Deep Q-Networks (DQN)</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Utilize neural networks to approximate Q-values for more complex environments.</p>
                                <p><strong>Scenario:</strong> Implement a DQN agent to solve the CartPole-v1 environment, leveraging TensorFlow or PyTorch for the neural network.</p>
                                <p><strong>Key Concepts:</strong> Deep Q-Learning, neural networks, experience replay.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">import gym
                import torch
                import torch.nn as nn
                import torch.optim as optim
                import random
                import numpy as np
                from collections import deque
                
                # Define the neural network
                class DQN(nn.Module):
                    def __init__(self, state_size, action_size):
                        super(DQN, self).__init__()
                        self.fc1 = nn.Linear(state_size, 24)
                        self.fc2 = nn.Linear(24, 24)
                        self.out = nn.Linear(24, action_size)
                
                    def forward(self, x):
                        x = torch.relu(self.fc1(x))
                        x = torch.relu(self.fc2(x))
                        return self.out(x)
                
                # Hyperparameters
                state_size = 4
                action_size = 2
                batch_size = 64
                gamma = 0.95
                epsilon = 1.0
                epsilon_decay = 0.995
                epsilon_min = 0.01
                learning_rate = 0.001
                memory = deque(maxlen=2000)
                
                env = gym.make('CartPole-v1')
                model = DQN(state_size, action_size)
                optimizer = optim.Adam(model.parameters(), lr=learning_rate)
                criterion = nn.MSELoss()
                
                # Experience replay
                def replay():
                    if len(memory) < batch_size:
                        return
                    minibatch = random.sample(memory, batch_size)
                    states, actions, rewards, next_states, dones = zip(*minibatch)
                
                    states = torch.FloatTensor(states)
                    actions = torch.LongTensor(actions).unsqueeze(1)
                    rewards = torch.FloatTensor(rewards)
                    next_states = torch.FloatTensor(next_states)
                    dones = torch.FloatTensor(dones)
                
                    q_values = model(states).gather(1, actions).squeeze()
                    next_q_values = model(next_states).max(1)[0]
                    targets = rewards + gamma * next_q_values * (1 - dones)
                
                    loss = criterion(q_values, targets)
                
                    optimizer.zero_grad()
                    loss.backward()
                    optimizer.step()
                
                # Training loop
                num_episodes = 1000
                for episode in range(num_episodes):
                    state = env.reset()
                    total_reward = 0
                    done = False
                
                    while not done:
                        state_tensor = torch.FloatTensor(state).unsqueeze(0)
                        if random.random() < epsilon:
                            action = env.action_space.sample()
                        else:
                            with torch.no_grad():
                                action = model(state_tensor).argmax().item()
                
                        next_state, reward, done, _ = env.step(action)
                        memory.append((state, action, reward, next_state, done))
                        state = next_state
                        total_reward += reward
                
                        replay()
                
                    epsilon = max(epsilon * epsilon_decay, epsilon_min)
                    if episode % 100 == 0:
                        print(f"Episode {episode}, Total Reward: {total_reward}")
                
                # Testing the trained agent
                state = env.reset()
                done = False
                total_reward = 0
                
                while not done:
                    state_tensor = torch.FloatTensor(state).unsqueeze(0)
                    with torch.no_grad():
                        action = model(state_tensor).argmax().item()
                    state, reward, done, _ = env.step(action)
                    total_reward += reward
                    env.render()
                
                print("Total Reward:", total_reward)</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 4: Policy Gradient Methods</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Implement policy-based reinforcement learning algorithms.</p>
                                <p><strong>Scenario:</strong> Develop a Policy Gradient agent to solve the CartPole-v1 environment, focusing on directly optimizing the policy.</p>
                                <p><strong>Key Concepts:</strong> Policy gradients, stochastic policies, gradient ascent.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">import gym
                import torch
                import torch.nn as nn
                import torch.optim as optim
                import numpy as np
                
                # Define the policy network
                class PolicyNet(nn.Module):
                    def __init__(self, state_size, action_size):
                        super(PolicyNet, self).__init__()
                        self.fc1 = nn.Linear(state_size, 128)
                        self.fc2 = nn.Linear(128, action_size)
                
                    def forward(self, x):
                        x = torch.relu(self.fc1(x))
                        return torch.softmax(self.fc2(x), dim=-1)
                
                env = gym.make('CartPole-v1')
                policy = PolicyNet(state_size=4, action_size=2)
                optimizer = optim.Adam(policy.parameters(), lr=1e-2)
                gamma = 0.99
                
                def select_action(state):
                    state = torch.FloatTensor(state).unsqueeze(0)
                    probs = policy(state)
                    action = np.random.choice(len(probs[0]), p=probs.detach().numpy()[0])
                    return action, torch.log(probs[0, action])
                
                def train():
                    rewards = []
                    log_probs = []
                    state = env.reset()
                    done = False
                    total_reward = 0
                
                    while not done:
                        action, log_prob = select_action(state)
                        next_state, reward, done, _ = env.step(action)
                        log_probs.append(log_prob)
                        rewards.append(reward)
                        state = next_state
                        total_reward += reward
                
                    discounted_rewards = []
                    R = 0
                    for r in reversed(rewards):
                        R = r + gamma * R
                        discounted_rewards.insert(0, R)
                    discounted_rewards = torch.FloatTensor(discounted_rewards)
                    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9)
                
                    loss = []
                    for log_prob, R in zip(log_probs, discounted_rewards):
                        loss.append(-log_prob * R)
                    loss = torch.cat(loss).sum()
                
                    optimizer.zero_grad()
                    loss.backward()
                    optimizer.step()
                
                    return total_reward
                
                # Training loop
                num_episodes = 1000
                for episode in range(num_episodes):
                    reward = train()
                    if episode % 100 == 0:
                        print(f"Episode {episode}, Total Reward: {reward}")
                
                # Testing the trained agent
                state = env.reset()
                done = False
                total_reward = 0
                
                while not done:
                    state_tensor = torch.FloatTensor(state).unsqueeze(0)
                    with torch.no_grad():
                        action = policy(state_tensor).argmax().item()
                    state, reward, done, _ = env.step(action)
                    total_reward += reward
                    env.render()
                
                print("Total Reward:", total_reward)</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 5: Actor-Critic Methods</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Combine value-based and policy-based methods for more efficient learning.</p>
                                <p><strong>Scenario:</strong> Implement an Actor-Critic agent to solve the CartPole-v1 environment, leveraging both policy and value networks.</p>
                                <p><strong>Key Concepts:</strong> Actor-Critic architecture, advantage function, simultaneous training.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">import gym
                import torch
                import torch.nn as nn
                import torch.optim as optim
                import numpy as np
                
                # Define the Actor network
                class Actor(nn.Module):
                    def __init__(self, state_size, action_size):
                        super(Actor, self).__init__()
                        self.fc1 = nn.Linear(state_size, 128)
                        self.fc2 = nn.Linear(128, action_size)
                
                    def forward(self, x):
                        x = torch.relu(self.fc1(x))
                        return torch.softmax(self.fc2(x), dim=-1)
                
                # Define the Critic network
                class Critic(nn.Module):
                    def __init__(self, state_size):
                        super(Critic, self).__init__()
                        self.fc1 = nn.Linear(state_size, 128)
                        self.fc2 = nn.Linear(128, 1)
                
                    def forward(self, x):
                        x = torch.relu(self.fc1(x))
                        return self.fc2(x)
                
                env = gym.make('CartPole-v1')
                actor = Actor(state_size=4, action_size=2)
                critic = Critic(state_size=4)
                actor_optimizer = optim.Adam(actor.parameters(), lr=1e-3)
                critic_optimizer = optim.Adam(critic.parameters(), lr=1e-3)
                gamma = 0.99
                
                def select_action(state):
                    state = torch.FloatTensor(state).unsqueeze(0)
                    probs = actor(state)
                    action = np.random.choice(len(probs[0]), p=probs.detach().numpy()[0])
                    return action, probs[0, action]
                
                def train():
                    states = []
                    actions = []
                    rewards = []
                    log_probs = []
                    state = env.reset()
                    done = False
                    total_reward = 0
                
                    while not done:
                        action, prob = select_action(state)
                        next_state, reward, done, _ = env.step(action)
                        states.append(state)
                        actions.append(action)
                        rewards.append(reward)
                        log_probs.append(torch.log(prob[action]))
                        state = next_state
                        total_reward += reward
                
                    # Compute returns
                    returns = []
                    R = 0
                    for r in reversed(rewards):
                        R = r + gamma * R
                        returns.insert(0, R)
                    returns = torch.FloatTensor(returns)
                    returns = (returns - returns.mean()) / (returns.std() + 1e-9)
                
                    # Update Critic
                    states_tensor = torch.FloatTensor(states)
                    values = critic(states_tensor).squeeze()
                    critic_loss = nn.MSELoss()(values, returns)
                    critic_optimizer.zero_grad()
                    critic_loss.backward()
                    critic_optimizer.step()
                
                    # Update Actor
                    advantage = returns - values.detach()
                    actor_loss = -torch.sum(torch.stack(log_probs) * advantage)
                    actor_optimizer.zero_grad()
                    actor_loss.backward()
                    actor_optimizer.step()
                
                    return total_reward
                
                # Training loop
                num_episodes = 1000
                for episode in range(num_episodes):
                    reward = train()
                    if episode % 100 == 0:
                        print(f"Episode {episode}, Total Reward: {reward}")
                
                # Testing the trained agent
                state = env.reset()
                done = False
                total_reward = 0
                
                while not done:
                    state_tensor = torch.FloatTensor(state).unsqueeze(0)
                    with torch.no_grad():
                        action = actor(state_tensor).argmax().item()
                    state, reward, done, _ = env.step(action)
                    total_reward += reward
                    env.render()
                
                print("Total Reward:", total_reward)</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 6: Exploring Exploration vs. Exploitation</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Balance exploration and exploitation to optimize learning.</p>
                                <p><strong>Scenario:</strong> Modify the epsilon-greedy strategy in a Q-Learning agent to implement an adaptive exploration rate based on performance.</p>
                                <p><strong>Key Concepts:</strong> Epsilon decay strategies, exploration rate adaptation.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">import gym
                import numpy as np
                
                env = gym.make('FrozenLake-v1', is_slippery=False)
                q_table = np.zeros([env.observation_space.n, env.action_space.n])
                
                alpha = 0.8
                gamma = 0.95
                epsilon = 1.0
                num_episodes = 2000
                performance_threshold = 195
                window_size = 100
                performance = deque(maxlen=window_size)
                
                for episode in range(num_episodes):
                    state = env.reset()
                    done = False
                    total_reward = 0
                
                    while not done:
                        if np.random.uniform(0, 1) < epsilon:
                            action = env.action_space.sample()
                        else:
                            action = np.argmax(q_table[state])
                
                        next_state, reward, done, _ = env.step(action)
                
                        q_table[state, action] = q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state]) - q_table[state, action])
                
                        state = next_state
                        total_reward += reward
                
                    performance.append(total_reward)
                    average_performance = np.mean(performance)
                
                    # Adaptive epsilon
                    if average_performance > performance_threshold:
                        epsilon = max(epsilon * 0.99, 0.01)
                    else:
                        epsilon = min(epsilon * 1.01, 1.0)
                
                    if episode % 100 == 0:
                        print(f"Episode {episode}, Average Reward: {average_performance}, Epsilon: {epsilon}")
                
                # Test the agent
                state = env.reset()
                done = False
                total_reward = 0
                
                while not done:
                    action = np.argmax(q_table[state])
                    state, reward, done, _ = env.step(action)
                    total_reward += reward
                    env.render()
                
                print("Total Reward:", total_reward)</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 7: Reward Shaping</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Enhance learning efficiency by designing effective reward structures.</p>
                                <p><strong>Scenario:</strong> Modify the reward function in the FrozenLake environment to provide intermediate rewards for reaching certain states, thereby accelerating the agent's learning process.</p>
                                <p><strong>Key Concepts:</strong> Reward engineering, intermediate rewards, shaping rewards.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">import gym
                import numpy as np
                
                class RewardShapingEnv(gym.Env):
                    def __init__(self):
                        self.env = gym.make('FrozenLake-v1', is_slippery=False)
                        self.observation_space = self.env.observation_space
                        self.action_space = self.env.action_space
                
                    def reset(self):
                        return self.env.reset()
                
                    def step(self, action):
                        next_state, reward, done, info = self.env.step(action)
                        # Add intermediate rewards
                        if next_state in [5, 7, 11, 12, 15]:
                            reward += 0.5
                        return next_state, reward, done, info
                
                env = RewardShapingEnv()
                q_table = np.zeros([env.observation_space.n, env.action_space.n])
                
                alpha = 0.8
                gamma = 0.95
                epsilon = 1.0
                num_episodes = 2000
                
                for episode in range(num_episodes):
                    state = env.reset()
                    done = False
                    total_reward = 0
                
                    while not done:
                        if np.random.uniform(0, 1) < epsilon:
                            action = env.action_space.sample()
                        else:
                            action = np.argmax(q_table[state])
                
                        next_state, reward, done, _ = env.step(action)
                
                        q_table[state, action] = q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state]) - q_table[state, action])
                
                        state = next_state
                        total_reward += reward
                
                    epsilon = max(epsilon * 0.995, 0.01)
                
                    if episode % 100 == 0:
                        print(f"Episode {episode}, Total Reward: {total_reward}")
                
                # Test the agent
                state = env.reset()
                done = False
                total_reward = 0
                
                while not done:
                    action = np.argmax(q_table[state])
                    state, reward, done, _ = env.step(action)
                    total_reward += reward
                    env.render()
                
                print("Total Reward:", total_reward)</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 8: Multi-Agent Reinforcement Learning</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Explore reinforcement learning in environments with multiple agents.</p>
                                <p><strong>Scenario:</strong> Implement a multi-agent Q-Learning setup where two agents cooperate to achieve a common goal in a gridworld environment.</p>
                                <p><strong>Key Concepts:</strong> Multi-agent systems, cooperative learning, shared rewards.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">import numpy as np
                
                class MultiAgentEnv:
                    def __init__(self, size=5):
                        self.size = size
                        self.reset()
                
                    def reset(self):
                        self.agent_positions = [0, self.size - 1]
                        self.goal = self.size // 2
                        return self.agent_positions
                
                    def step(self, actions):
                        rewards = [0, 0]
                        done = False
                
                        for i, action in enumerate(actions):
                            if action == 0 and self.agent_positions[i] > 0:
                                self.agent_positions[i] -= 1
                            elif action == 1 and self.agent_positions[i] < self.size - 1:
                                self.agent_positions[i] += 1
                
                        if all(pos == self.goal for pos in self.agent_positions):
                            rewards = [10, 10]
                            done = True
                        else:
                            rewards = [-1, -1]
                
                        return self.agent_positions, rewards, done
                
                env = MultiAgentEnv()
                q_tables = [np.zeros(env.size) for _ in range(2)]
                alpha = 0.1
                gamma = 0.9
                epsilon = 0.1
                num_episodes = 500
                
                for episode in range(num_episodes):
                    state = env.reset()
                    done = False
                
                    while not done:
                        actions = []
                        for i in range(2):
                            if np.random.uniform(0, 1) < epsilon:
                                action = np.random.choice([0, 1])
                            else:
                                action = np.argmax(q_tables[i][state[i]:state[i]+1])
                            actions.append(action)
                
                        next_state, rewards, done = env.step(actions)
                
                        for i in range(2):
                            best_next = np.max(q_tables[i][next_state[i]:next_state[i]+1])
                            q_tables[i][state[i]] += alpha * (rewards[i] + gamma * best_next - q_tables[i][state[i]])
                
                        state = next_state
                
                    if episode % 100 == 0:
                        print(f"Episode {episode} completed.")
                
                # Test the agents
                state = env.reset()
                done = False
                total_rewards = [0, 0]
                
                while not done:
                    actions = []
                    for i in range(2):
                        action = np.argmax(q_tables[i][state[i]:state[i]+1])
                        actions.append(action)
                    state, rewards, done = env.step(actions)
                    total_rewards = [sum(x) for x in zip(total_rewards, rewards)]
                    print(f"Agent 1 Position: {state[0]}, Agent 2 Position: {state[1]}")
                
                print("Total Rewards:", total_rewards)</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 9: Transfer Learning in Reinforcement Learning</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Apply knowledge from one task to accelerate learning in another.</p>
                                <p><strong>Scenario:</strong> Use a pre-trained Q-Table from the FrozenLake environment to initialize the Q-Table for a modified environment with additional obstacles.</p>
                                <p><strong>Key Concepts:</strong> Transfer learning, knowledge reuse, environment adaptation.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">import gym
                import numpy as np
                
                # Original environment
                original_env = gym.make('FrozenLake-v1', is_slippery=False)
                original_q_table = np.zeros([original_env.observation_space.n, original_env.action_space.n])
                
                # Train on original environment
                alpha = 0.8
                gamma = 0.95
                epsilon = 1.0
                num_episodes = 2000
                
                for episode in range(num_episodes):
                    state = original_env.reset()
                    done = False
                
                    while not done:
                        if np.random.uniform(0, 1) < epsilon:
                            action = original_env.action_space.sample()
                        else:
                            action = np.argmax(original_q_table[state])
                
                        next_state, reward, done, _ = original_env.step(action)
                
                        original_q_table[state, action] = original_q_table[state, action] + alpha * (reward + gamma * np.max(original_q_table[next_state]) - original_q_table[state, action])
                
                        state = next_state
                
                    epsilon = max(epsilon * 0.995, 0.01)
                
                    if episode % 500 == 0:
                        print(f"Original Env - Episode {episode}")
                
                # Modified environment with additional obstacles
                class ModifiedFrozenLakeEnv(gym.Env):
                    def __init__(self):
                        self.env = gym.make('FrozenLake-v1', is_slippery=False)
                        self.observation_space = self.env.observation_space
                        self.action_space = self.env.action_space
                        # Add additional obstacles by modifying the map
                        self.env.unwrapped.desc = np.array([
                            list("SFFF"),
                            list("FHFH"),
                            list("FFFH"),
                            list("HFFG")
                        ])
                
                    def reset(self):
                        return self.env.reset()
                
                    def step(self, action):
                        return self.env.step(action)
                
                modified_env = ModifiedFrozenLakeEnv()
                modified_q_table = original_q_table.copy()  # Transfer Q-Table
                
                # Train on modified environment
                epsilon = 1.0
                for episode in range(num_episodes):
                    state = modified_env.reset()
                    done = False
                
                    while not done:
                        if np.random.uniform(0, 1) < epsilon:
                            action = modified_env.action_space.sample()
                        else:
                            action = np.argmax(modified_q_table[state])
                
                        next_state, reward, done, _ = modified_env.step(action)
                
                        modified_q_table[state, action] = modified_q_table[state, action] + alpha * (reward + gamma * np.max(modified_q_table[next_state]) - modified_q_table[state, action])
                
                        state = next_state
                
                    epsilon = max(epsilon * 0.995, 0.01)
                
                    if episode % 500 == 0:
                        print(f"Modified Env - Episode {episode}")
                
                # Test the agent in modified environment
                state = modified_env.reset()
                done = False
                total_reward = 0
                
                while not done:
                    action = np.argmax(modified_q_table[state])
                    state, reward, done, _ = modified_env.step(action)
                    total_reward += reward
                    modified_env.env.render()
                
                print("Total Reward in Modified Env:", total_reward)</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 10: Implementing Proximal Policy Optimization (PPO)</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Apply advanced policy optimization techniques for stable and efficient learning.</p>
                                <p><strong>Scenario:</strong> Implement the PPO algorithm to train an agent in the CartPole-v1 environment, ensuring clipped policy updates for stability.</p>
                                <p><strong>Key Concepts:</strong> Proximal Policy Optimization, surrogate loss, clipped objective.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">import gym
                import torch
                import torch.nn as nn
                import torch.optim as optim
                from torch.distributions import Categorical
                
                # Define the PPO Actor-Critic network
                class PPOActorCritic(nn.Module):
                    def __init__(self, state_size, action_size):
                        super(PPOActorCritic, self).__init__()
                        self.fc1 = nn.Linear(state_size, 64)
                        self.actor = nn.Linear(64, action_size)
                        self.critic = nn.Linear(64, 1)
                
                    def forward(self, x):
                        x = torch.relu(self.fc1(x))
                        action_probs = torch.softmax(self.actor(x), dim=-1)
                        state_value = self.critic(x)
                        return action_probs, state_value
                
                env = gym.make('CartPole-v1')
                model = PPOActorCritic(state_size=4, action_size=2)
                optimizer = optim.Adam(model.parameters(), lr=1e-3)
                eps_clip = 0.2
                gamma = 0.99
                
                def select_action(state):
                    state = torch.FloatTensor(state).unsqueeze(0)
                    probs, state_value = model(state)
                    m = Categorical(probs)
                    action = m.sample()
                    return action.item(), m.log_prob(action), state_value
                
                def compute_returns(rewards, dones, next_value):
                    returns = []
                    R = next_value
                    for r, d in zip(reversed(rewards), reversed(dones)):
                        R = r + gamma * R * (1 - d)
                        returns.insert(0, R)
                    return returns
                
                num_epochs = 1000
                for epoch in range(num_epochs):
                    state = env.reset()
                    log_probs = []
                    values = []
                    rewards = []
                    dones = []
                    actions = []
                
                    # Collect trajectory
                    for _ in range(200):
                        action, log_prob, value = select_action(state)
                        next_state, reward, done, _ = env.step(action)
                
                        log_probs.append(log_prob)
                        values.append(value)
                        rewards.append(reward)
                        dones.append(done)
                        actions.append(action)
                
                        state = next_state
                        if done:
                            break
                
                    _, next_value = model(torch.FloatTensor(state).unsqueeze(0))
                    returns = compute_returns(rewards, dones, next_value.item())
                
                    # Convert to tensors
                    log_probs = torch.stack(log_probs)
                    values = torch.stack(values).squeeze()
                    returns = torch.tensor(returns)
                    advantage = returns - values
                
                    # PPO Loss
                    probs, state_values = model(torch.FloatTensor(state).unsqueeze(0))
                    distribution = Categorical(probs)
                    new_log_probs = distribution.log_prob(torch.tensor(actions))
                    ratio = (new_log_probs - log_probs.detach()).exp()
                    surr1 = ratio * advantage
                    surr2 = torch.clamp(ratio, 1 - eps_clip, 1 + eps_clip) * advantage
                    loss = -torch.min(surr1, surr2).mean() + nn.MSELoss()(state_values.squeeze(), returns)
                
                    # Optimize
                    optimizer.zero_grad()
                    loss.backward()
                    optimizer.step()
                
                    if epoch % 100 == 0:
                        print(f"Epoch {epoch}, Loss: {loss.item()}")
                
                # Test the trained agent
                state = env.reset()
                done = False
                total_reward = 0
                
                while not done:
                    state_tensor = torch.FloatTensor(state).unsqueeze(0)
                    with torch.no_grad():
                        probs, _ = model(state_tensor)
                    action = torch.argmax(probs).item()
                    state, reward, done, _ = env.step(action)
                    total_reward += reward
                    env.render()
                
                print("Total Reward with PPO:", total_reward)</pre>
                            </div>
                        </li>
                
                    </ul>
                </div>
                



                <!-- Functions -->

                <div id="case-study3" class="main-content3">
                    <h2 id="deep-learning-section">Deep Learning</h2><br>
                    <ul class="chapter-list">
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 1: Building a Simple Neural Network with TensorFlow</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Understand the basics of building a neural network using TensorFlow.</p>
                                <p><strong>Scenario:</strong> Create a simple neural network to perform binary classification on the Iris dataset.</p>
                                <p><strong>Key Concepts:</strong> TensorFlow basics, neural network architecture, binary classification.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">import tensorflow as tf
                from sklearn.datasets import load_iris
                from sklearn.model_selection import train_test_split
                from sklearn.preprocessing import OneHotEncoder
                
                # Load and preprocess the Iris dataset
                iris = load_iris()
                X = iris.data
                y = iris.target.reshape(-1, 1)
                
                # Convert to binary classification (Setosa vs. others)
                y = (y == 0).astype(int)
                
                encoder = OneHotEncoder(sparse=False)
                y = encoder.fit_transform(y)
                
                # Split the dataset
                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
                
                # Build the neural network model
                model = tf.keras.Sequential([
                    tf.keras.layers.Dense(10, activation='relu', input_shape=(X_train.shape[1],)),
                    tf.keras.layers.Dense(10, activation='relu'),
                    tf.keras.layers.Dense(2, activation='softmax')
                ])
                
                # Compile the model
                model.compile(optimizer='adam',
                              loss='categorical_crossentropy',
                              metrics=['accuracy'])
                
                # Train the model
                model.fit(X_train, y_train, epochs=50, batch_size=16, validation_split=0.2)
                
                # Evaluate the model
                loss, accuracy = model.evaluate(X_test, y_test)
                print("Test Accuracy:", accuracy)</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 2: Image Classification with Convolutional Neural Networks (CNNs)</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Learn how to build and train a CNN for image classification tasks.</p>
                                <p><strong>Scenario:</strong> Develop a CNN to classify images from the CIFAR-10 dataset into 10 categories.</p>
                                <p><strong>Key Concepts:</strong> Convolutional layers, pooling layers, CNN architecture, image classification.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">import tensorflow as tf
                from tensorflow.keras import datasets, layers, models
                import matplotlib.pyplot as plt
                
                # Load and preprocess CIFAR-10 dataset
                (train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()
                
                # Normalize pixel values
                train_images, test_images = train_images / 255.0, test_images / 255.0
                
                # Define class names
                class_names = ['Airplane', 'Car', 'Bird', 'Cat', 'Deer',
                               'Dog', 'Frog', 'Horse', 'Ship', 'Truck']
                
                # Build the CNN model
                model = models.Sequential([
                    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),
                    layers.MaxPooling2D((2, 2)),
                    
                    layers.Conv2D(64, (3, 3), activation='relu'),
                    layers.MaxPooling2D((2, 2)),
                    
                    layers.Conv2D(64, (3, 3), activation='relu'),
                    
                    layers.Flatten(),
                    layers.Dense(64, activation='relu'),
                    layers.Dense(10, activation='softmax')
                ])
                
                # Compile the model
                model.compile(optimizer='adam',
                              loss='sparse_categorical_crossentropy',
                              metrics=['accuracy'])
                
                # Train the model
                history = model.fit(train_images, train_labels, epochs=10,
                                    validation_data=(test_images, test_labels))
                
                # Evaluate the model
                plt.plot(history.history['accuracy'], label='accuracy')
                plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
                plt.xlabel('Epoch')
                plt.ylabel('Accuracy')
                plt.ylim([0, 1])
                plt.legend(loc='lower right')
                
                test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)
                print("Test Accuracy:", test_acc)</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 3: Implementing a Recurrent Neural Network (RNN) for Text Generation</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Explore the use of RNNs for sequential data processing and text generation.</p>
                                <p><strong>Scenario:</strong> Build an RNN to generate text based on the works of Shakespeare.</p>
                                <p><strong>Key Concepts:</strong> Recurrent layers, sequence modeling, text generation.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">import tensorflow as tf
                import numpy as np
                import os
                import time
                
                # Download and preprocess the text data
                path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')
                
                text = open(path_to_file, 'rb').read().decode(encoding='utf-8')
                print(f'Length of text: {len(text)} characters')
                
                # Create a mapping from unique characters to indices
                vocab = sorted(set(text))
                char2idx = {u:i for i, u in enumerate(vocab)}
                idx2char = np.array(vocab)
                
                # Convert text to integers
                text_as_int = np.array([char2idx[c] for c in text])
                
                # Create training examples / targets
                seq_length = 100
                examples_per_epoch = len(text)//(seq_length + 1)
                
                char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)
                
                sequences = char_dataset.batch(seq_length+1, drop_remainder=True)
                
                def split_input_target(chunk):
                    input_text = chunk[:-1]
                    target_text = chunk[1:]
                    return input_text, target_text
                
                dataset = sequences.map(split_input_target)
                
                # Batch size
                BATCH_SIZE = 64
                BUFFER_SIZE = 10000
                
                dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)
                
                # Build the RNN model
                vocab_size = len(vocab)
                embedding_dim = 256
                rnn_units = 1024
                
                model = tf.keras.Sequential([
                    tf.keras.layers.Embedding(vocab_size, embedding_dim,
                                              batch_input_shape=[BATCH_SIZE, None]),
                    tf.keras.layers.GRU(rnn_units,
                                        return_sequences=True,
                                        stateful=True,
                                        recurrent_initializer='glorot_uniform'),
                    tf.keras.layers.Dense(vocab_size)
                ])
                
                # Define the loss
                def loss(labels, logits):
                    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)
                
                model.compile(optimizer='adam', loss=loss)
                
                # Configure checkpoints
                checkpoint_dir = './training_checkpoints'
                checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt_{epoch}")
                
                checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
                    filepath=checkpoint_prefix,
                    save_weights_only=True)
                
                # Train the model
                history = model.fit(dataset, epochs=10, callbacks=[checkpoint_callback])
                
                # Text generation
                def generate_text(model, start_string):
                    num_generate = 1000
                    input_eval = [char2idx[s] for s in start_string]
                    input_eval = tf.expand_dims(input_eval, 0)
                    text_generated = []
                    
                    model.reset_states()
                    for i in range(num_generate):
                        predictions = model(input_eval)
                        predictions = tf.squeeze(predictions, 0)
                        predictions = predictions[-1] / 1.0  # Temperature
                        predicted_id = tf.random.categorical(predictions[tf.newaxis, ...], num_samples=1)[-1,0].numpy()
                        input_eval = tf.expand_dims([predicted_id], 0)
                        text_generated.append(idx2char[predicted_id])
                    
                    return start_string + ''.join(text_generated)
                
                # Load the model with batch size 1 for generation
                model = tf.keras.Sequential([
                    tf.keras.layers.Embedding(vocab_size, embedding_dim,
                                              batch_input_shape=[1, None]),
                    tf.keras.layers.GRU(rnn_units,
                                        return_sequences=True,
                                        stateful=True,
                                        recurrent_initializer='glorot_uniform'),
                    tf.keras.layers.Dense(vocab_size)
                ])
                
                model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))
                model.build(tf.TensorShape([1, None]))
                
                print(generate_text(model, start_string="ROMEO: "))</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 4: Transfer Learning with Pre-trained Models</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Utilize pre-trained models to perform transfer learning for image classification.</p>
                                <p><strong>Scenario:</strong> Fine-tune a pre-trained VGG16 model on the Cats vs. Dogs dataset.</p>
                                <p><strong>Key Concepts:</strong> Transfer learning, fine-tuning, pre-trained models.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">import tensorflow as tf
                from tensorflow.keras.applications import VGG16
                from tensorflow.keras import layers, models
                from tensorflow.keras.preprocessing.image import ImageDataGenerator
                
                # Load the pre-trained VGG16 model without the top layers
                base_model = VGG16(weights='imagenet', include_top=False, input_shape=(150, 150, 3))
                
                # Freeze the base model
                base_model.trainable = False
                
                # Add custom layers on top
                model = models.Sequential([
                    base_model,
                    layers.Flatten(),
                    layers.Dense(256, activation='relu'),
                    layers.Dropout(0.5),
                    layers.Dense(1, activation='sigmoid')
                ])
                
                # Compile the model
                model.compile(optimizer='adam',
                              loss='binary_crossentropy',
                              metrics=['accuracy'])
                
                # Prepare data generators
                train_datagen = ImageDataGenerator(rescale=1./255,
                                                   rotation_range=40,
                                                   width_shift_range=0.2,
                                                   height_shift_range=0.2,
                                                   shear_range=0.2,
                                                   zoom_range=0.2,
                                                   horizontal_flip=True,
                                                   fill_mode='nearest')
                
                test_datagen = ImageDataGenerator(rescale=1./255)
                
                train_generator = train_datagen.flow_from_directory(
                    'data/train',
                    target_size=(150, 150),
                    batch_size=20,
                    class_mode='binary')
                
                validation_generator = test_datagen.flow_from_directory(
                    'data/validation',
                    target_size=(150, 150),
                    batch_size=20,
                    class_mode='binary')
                
                # Train the model
                history = model.fit(
                    train_generator,
                    steps_per_epoch=100,
                    epochs=30,
                    validation_data=validation_generator,
                    validation_steps=50)
                
                # Unfreeze some layers and fine-tune
                base_model.trainable = True
                model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),
                              loss='binary_crossentropy',
                              metrics=['accuracy'])
                
                history_fine = model.fit(
                    train_generator,
                    steps_per_epoch=100,
                    epochs=10,
                    validation_data=validation_generator,
                    validation_steps=50)</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 5: Implementing Regularization Techniques</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Apply regularization methods to prevent overfitting in deep learning models.</p>
                                <p><strong>Scenario:</strong> Enhance a neural network for MNIST digit classification by adding dropout and L2 regularization.</p>
                                <p><strong>Key Concepts:</strong> Dropout, L2 regularization, overfitting prevention.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">import tensorflow as tf
                from tensorflow.keras import datasets, layers, models, regularizers
                import matplotlib.pyplot as plt
                
                # Load and preprocess MNIST dataset
                (train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()
                train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255
                test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255
                
                # Build the model with dropout and L2 regularization
                model = models.Sequential([
                    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1),
                                  kernel_regularizer=regularizers.l2(0.001)),
                    layers.MaxPooling2D((2,2)),
                    layers.Dropout(0.25),
                    
                    layers.Conv2D(64, (3,3), activation='relu',
                                  kernel_regularizer=regularizers.l2(0.001)),
                    layers.MaxPooling2D((2,2)),
                    layers.Dropout(0.25),
                    
                    layers.Flatten(),
                    layers.Dense(128, activation='relu',
                                 kernel_regularizer=regularizers.l2(0.001)),
                    layers.Dropout(0.5),
                    layers.Dense(10, activation='softmax')
                ])
                
                # Compile the model
                model.compile(optimizer='adam',
                              loss='sparse_categorical_crossentropy',
                              metrics=['accuracy'])
                
                # Train the model
                history = model.fit(train_images, train_labels, epochs=20,
                                    batch_size=128,
                                    validation_split=0.2)
                
                # Evaluate the model
                plt.plot(history.history['accuracy'], label='accuracy')
                plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
                plt.xlabel('Epoch')
                plt.ylabel('Accuracy')
                plt.legend(loc='lower right')
                
                test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)
                print("Test Accuracy:", test_acc)</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 6: Utilizing Transfer Learning with PyTorch</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Implement transfer learning using a pre-trained model in PyTorch.</p>
                                <p><strong>Scenario:</strong> Fine-tune a pre-trained ResNet18 model on a custom dataset for flower classification.</p>
                                <p><strong>Key Concepts:</strong> Transfer learning, PyTorch models, fine-tuning.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">import torch
                import torch.nn as nn
                import torch.optim as optim
                from torchvision import datasets, transforms, models
                
                # Define transformations
                transform = transforms.Compose([
                    transforms.Resize(224),
                    transforms.CenterCrop(224),
                    transforms.ToTensor(),
                    transforms.Normalize([0.485, 0.456, 0.406],
                                         [0.229, 0.224, 0.225])
                ])
                
                # Load datasets
                train_dataset = datasets.ImageFolder('data/flowers/train', transform=transform)
                val_dataset = datasets.ImageFolder('data/flowers/val', transform=transform)
                
                train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)
                val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)
                
                # Load pre-trained ResNet18 model
                model = models.resnet18(pretrained=True)
                
                # Freeze all layers
                for param in model.parameters():
                    param.requires_grad = False
                
                # Modify the final layer
                num_ftrs = model.fc.in_features
                model.fc = nn.Linear(num_ftrs, len(train_dataset.classes))
                
                # Move the model to GPU if available
                device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
                model = model.to(device)
                
                # Define loss and optimizer
                criterion = nn.CrossEntropyLoss()
                optimizer = optim.Adam(model.fc.parameters(), lr=0.001)
                
                # Training loop
                num_epochs = 10
                for epoch in range(num_epochs):
                    model.train()
                    running_loss = 0.0
                    for inputs, labels in train_loader:
                        inputs, labels = inputs.to(device), labels.to(device)
                        
                        optimizer.zero_grad()
                        outputs = model(inputs)
                        loss = criterion(outputs, labels)
                        loss.backward()
                        optimizer.step()
                        
                        running_loss += loss.item() * inputs.size(0)
                    
                    epoch_loss = running_loss / len(train_loader.dataset)
                    
                    # Validation
                    model.eval()
                    correct = 0
                    total = 0
                    with torch.no_grad():
                        for inputs, labels in val_loader:
                            inputs, labels = inputs.to(device), labels.to(device)
                            outputs = model(inputs)
                            _, preds = torch.max(outputs, 1)
                            correct += (preds == labels).sum().item()
                            total += labels.size(0)
                    
                    epoch_acc = correct / total
                    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Validation Accuracy: {epoch_acc:.4f}')
                
                # Save the fine-tuned model
                torch.save(model.state_dict(), 'resnet18_finetuned.pth')</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 7: Implementing Dropout in Neural Networks</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Apply dropout regularization to prevent overfitting in neural networks.</p>
                                <p><strong>Scenario:</strong> Enhance a neural network for the MNIST dataset by adding dropout layers.</p>
                                <p><strong>Key Concepts:</strong> Dropout, regularization, overfitting prevention.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">import tensorflow as tf
                from tensorflow.keras import datasets, layers, models
                import matplotlib.pyplot as plt
                
                # Load and preprocess MNIST dataset
                (train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()
                train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255
                test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255
                
                # Build the model with dropout layers
                model = models.Sequential([
                    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),
                    layers.MaxPooling2D((2,2)),
                    layers.Dropout(0.25),
                    
                    layers.Conv2D(64, (3,3), activation='relu'),
                    layers.MaxPooling2D((2,2)),
                    layers.Dropout(0.25),
                    
                    layers.Flatten(),
                    layers.Dense(128, activation='relu'),
                    layers.Dropout(0.5),
                    layers.Dense(10, activation='softmax')
                ])
                
                # Compile the model
                model.compile(optimizer='adam',
                              loss='sparse_categorical_crossentropy',
                              metrics=['accuracy'])
                
                # Train the model
                history = model.fit(train_images, train_labels, epochs=20,
                                    batch_size=128,
                                    validation_split=0.2)
                
                # Evaluate the model
                plt.plot(history.history['accuracy'], label='accuracy')
                plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
                plt.xlabel('Epoch')
                plt.ylabel('Accuracy')
                plt.legend(loc='lower right')
                
                test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)
                print("Test Accuracy with Dropout:", test_acc)</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 8: Building a Generative Adversarial Network (GAN)</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Understand the architecture and training of GANs for image generation.</p>
                                <p><strong>Scenario:</strong> Implement a simple GAN to generate handwritten digits similar to those in the MNIST dataset.</p>
                                <p><strong>Key Concepts:</strong> GAN architecture, generator and discriminator, adversarial training.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">import tensorflow as tf
                from tensorflow.keras import layers
                import numpy as np
                import matplotlib.pyplot as plt
                
                # Load and preprocess MNIST dataset
                (train_images, _), (_, _) = tf.keras.datasets.mnist.load_data()
                train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')
                train_images = (train_images - 127.5) / 127.5  # Normalize to [-1, 1]
                
                BUFFER_SIZE = 60000
                BATCH_SIZE = 256
                EPOCHS = 50
                noise_dim = 100
                num_examples_to_generate = 16
                
                # Batch and shuffle the data
                train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
                
                # Build the generator
                def make_generator_model():
                    model = tf.keras.Sequential()
                    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(noise_dim,)))
                    model.add(layers.BatchNormalization())
                    model.add(layers.LeakyReLU())
                    
                    model.add(layers.Reshape((7, 7, 256)))
                    assert model.output_shape == (None, 7, 7, 256)
                    
                    model.add(layers.Conv2DTranspose(128, (5,5), strides=(1,1), padding='same', use_bias=False))
                    assert model.output_shape == (None, 7, 7, 128)
                    model.add(layers.BatchNormalization())
                    model.add(layers.LeakyReLU())
                    
                    model.add(layers.Conv2DTranspose(64, (5,5), strides=(2,2), padding='same', use_bias=False))
                    assert model.output_shape == (None, 14, 14, 64)
                    model.add(layers.BatchNormalization())
                    model.add(layers.LeakyReLU())
                    
                    model.add(layers.Conv2DTranspose(1, (5,5), strides=(2,2), padding='same', use_bias=False, activation='tanh'))
                    assert model.output_shape == (None, 28, 28, 1)
                    
                    return model
                
                # Build the discriminator
                def make_discriminator_model():
                    model = tf.keras.Sequential()
                    model.add(layers.Conv2D(64, (5,5), strides=(2,2), padding='same',
                                                     input_shape=[28, 28, 1]))
                    model.add(layers.LeakyReLU())
                    model.add(layers.Dropout(0.3))
                    
                    model.add(layers.Conv2D(128, (5,5), strides=(2,2), padding='same'))
                    model.add(layers.LeakyReLU())
                    model.add(layers.Dropout(0.3))
                    
                    model.add(layers.Flatten())
                    model.add(layers.Dense(1))
                    
                    return model
                
                generator = make_generator_model()
                discriminator = make_discriminator_model()
                
                # Define loss and optimizers
                cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)
                
                def discriminator_loss(real_output, fake_output):
                    real_loss = cross_entropy(tf.ones_like(real_output), real_output)
                    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
                    total_loss = real_loss + fake_loss
                    return total_loss
                
                def generator_loss(fake_output):
                    return cross_entropy(tf.ones_like(fake_output), fake_output)
                
                generator_optimizer = tf.keras.optimizers.Adam(1e-4)
                discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)
                
                # Training step
                @tf.function
                def train_step(images):
                    noise = tf.random.normal([BATCH_SIZE, noise_dim])
                    
                    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
                        generated_images = generator(noise, training=True)
                        
                        real_output = discriminator(images, training=True)
                        fake_output = discriminator(generated_images, training=True)
                        
                        gen_loss = generator_loss(fake_output)
                        disc_loss = discriminator_loss(real_output, fake_output)
                        
                    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)
                    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)
                    
                    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
                    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))
                
                # Generate and save images
                def generate_and_save_images(model, epoch, test_input):
                    predictions = model(test_input, training=False)
                    
                    fig = plt.figure(figsize=(4,4))
                    
                    for i in range(predictions.shape[0]):
                        plt.subplot(4, 4, i+1)
                        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')
                        plt.axis('off')
                    
                    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))
                    plt.close()
                
                # Training loop
                import os
                if not os.path.exists('generated_images'):
                    os.makedirs('generated_images')
                
                seed = tf.random.normal([num_examples_to_generate, noise_dim])
                
                for epoch in range(EPOCHS):
                    for image_batch in train_dataset:
                        train_step(image_batch)
                    
                    # Produce images for the GIF as we go
                    generate_and_save_images(generator, epoch + 1, seed)
                    
                    print(f'Epoch {epoch+1} completed.')
                
                # Generate final images
                generate_and_save_images(generator, EPOCHS, seed)</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 9: Hyperparameter Tuning with Keras Tuner</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Optimize neural network hyperparameters using Keras Tuner.</p>
                                <p><strong>Scenario:</strong> Use Keras Tuner to find the best number of layers and neurons for a model on the MNIST dataset.</p>
                                <p><strong>Key Concepts:</strong> Hyperparameter tuning, Keras Tuner, model optimization.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">import tensorflow as tf
                from tensorflow import keras
                from tensorflow.keras import layers
                import keras_tuner as kt
                
                # Load and preprocess MNIST dataset
                (train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()
                train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255
                test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255
                
                # Define the model-building function for Keras Tuner
                def build_model(hp):
                    model = keras.Sequential()
                    model.add(layers.Flatten(input_shape=(28,28,1)))
                    
                    # Tune the number of layers
                    for i in range(hp.Int('num_layers', 1, 3)):
                        model.add(layers.Dense(units=hp.Int(f'units_{i}', min_value=32, max_value=512, step=32),
                                               activation='relu'))
                        model.add(layers.Dropout(rate=hp.Float(f'dropout_{i}', 0.0, 0.5, step=0.1)))
                    
                    model.add(layers.Dense(10, activation='softmax'))
                    
                    # Tune the learning rate for the optimizer
                    model.compile(
                        optimizer=keras.optimizers.Adam(
                            hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),
                        loss='sparse_categorical_crossentropy',
                        metrics=['accuracy'])
                    
                    return model
                
                # Initialize the tuner
                tuner = kt.Hyperband(build_model,
                                     objective='val_accuracy',
                                     max_epochs=10,
                                     factor=3,
                                     directory='my_dir',
                                     project_name='mnist_tuning')
                
                # Define early stopping
                stop_early = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)
                
                # Perform hyperparameter search
                tuner.search(train_images, train_labels, epochs=50, validation_split=0.2, callbacks=[stop_early])
                
                # Get the optimal hyperparameters
                best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]
                print(f"""
                The hyperparameter search is complete. The optimal number of layers is {best_hps.get('num_layers')} and the optimal learning rate for the optimizer
                is {best_hps.get('learning_rate')}.
                """)
                
                # Build the model with the optimal hyperparameters and train it
                model = tuner.hypermodel.build(best_hps)
                history = model.fit(train_images, train_labels, epochs=20, validation_split=0.2)
                
                # Evaluate the model
                test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)
                print("Test Accuracy:", test_acc)</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 10: Deploying a Deep Learning Model with TensorFlow Serving</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Deploy a trained deep learning model for serving predictions in production.</p>
                                <p><strong>Scenario:</strong> Deploy a trained TensorFlow model using TensorFlow Serving and make prediction requests.</p>
                                <p><strong>Key Concepts:</strong> Model serving, TensorFlow Serving, REST API.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution"># Step 1: Save the trained TensorFlow model in the SavedModel format
                model.save('saved_model/my_model')
                
                # Step 2: Install TensorFlow Serving (Assuming Docker is installed)
                # Run the following command in the terminal to start TensorFlow Serving with the saved model:
                # docker run -p 8501:8501 --name=my_model_serving \
                #   --mount type=bind,source=$(pwd)/saved_model/my_model,target=/models/my_model \
                #   -e MODEL_NAME=my_model -t tensorflow/serving
                
                # Step 3: Make a prediction request using curl
                # Create a JSON file named 'request.json' with the input data:
                # {
                #   "instances": [[28, 28, 1, ...]]  # Replace with actual input data
                # }
                
                # Make the request:
                # curl -X POST http://localhost:8501/v1/models/my_model:predict -d @request.json
                
                # Example using Python to send a request
                import json
                import requests
                import numpy as np
                
                # Example input data (replace with actual data)
                input_data = np.random.rand(1, 28, 28, 1).tolist()
                
                # Create the payload
                payload = {"instances": input_data}
                
                # Send the request
                response = requests.post('http://localhost:8501/v1/models/my_model:predict', json=payload)
                
                # Print the prediction results
                print("Prediction:", response.json())</pre>
                            </div>
                        </li>
                
                    </ul>
                </div>
                


                <!-- Objexts and its use -->


                <div id="case-study3" class="main-content3">
                    <h2 id="objects-section">Natural Language Processing (NLP)</h2><br>
                    <ul class="chapter-list">
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 1: Tokenization</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Learn how to break down text into smaller units (tokens).</p>
                                <p><strong>Scenario:</strong> Write a Python script using the NLTK library to tokenize a given sentence into words.</p>
                                <p><strong>Key Concepts:</strong> Tokenization, word-level tokens, sentence splitting.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution"># Tokenizing text using NLTK
                import nltk
                nltk.download('punkt')
                from nltk.tokenize import word_tokenize
                
                # Sample sentence
                sentence = "Natural Language Processing is fun!"
                tokens = word_tokenize(sentence)
                print("Tokens:", tokens)</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 2: Part-of-Speech Tagging</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Understand how to assign part-of-speech tags to words.</p>
                                <p><strong>Scenario:</strong> Write a Python script to tag words with their part-of-speech using NLTK.</p>
                                <p><strong>Key Concepts:</strong> POS tagging, word categories, NLTK POS tagger.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution"># Part-of-Speech tagging using NLTK
                import nltk
                nltk.download('averaged_perceptron_tagger')
                
                # Sample sentence
                sentence = "NLP can be challenging."
                tokens = nltk.word_tokenize(sentence)
                tagged = nltk.pos_tag(tokens)
                print("POS Tags:", tagged)</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 3: Named Entity Recognition (NER)</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Learn how to identify named entities in text.</p>
                                <p><strong>Scenario:</strong> Write a Python script using the `spacy` library to perform named entity recognition on a sentence.</p>
                                <p><strong>Key Concepts:</strong> Named entities, entity categories, NER models.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution"># Named Entity Recognition using spaCy
                import spacy
                
                # Load the spacy model
                nlp = spacy.load('en_core_web_sm')
                
                # Sample sentence
                doc = nlp("Apple is looking at buying U.K. startup for $1 billion.")
                for ent in doc.ents:
                    print(ent.text, ent.label_)</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 4: Text Preprocessing</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Learn how to clean and prepare text data for NLP tasks.</p>
                                <p><strong>Scenario:</strong> Write a Python script to perform common text preprocessing tasks like lowercasing, removing stopwords, and stemming.</p>
                                <p><strong>Key Concepts:</strong> Text preprocessing, stopword removal, stemming, lemmatization.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution"># Text Preprocessing using NLTK
                from nltk.corpus import stopwords
                from nltk.stem import PorterStemmer
                from nltk.tokenize import word_tokenize
                nltk.download('stopwords')
                
                # Sample text
                text = "This is a simple example to demonstrate text preprocessing."
                
                # Lowercasing
                text = text.lower()
                
                # Tokenization
                tokens = word_tokenize(text)
                
                # Stopword removal
                stop_words = set(stopwords.words('english'))
                filtered_tokens = [word for word in tokens if word not in stop_words]
                
                # Stemming
                stemmer = PorterStemmer()
                stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]
                
                print("Processed tokens:", stemmed_tokens)</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 5: Sentiment Analysis</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Learn how to perform sentiment analysis on text data.</p>
                                <p><strong>Scenario:</strong> Write a Python script using the `TextBlob` library to determine the sentiment of a sentence.</p>
                                <p><strong>Key Concepts:</strong> Sentiment polarity, subjectivity, sentiment models.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution"># Sentiment analysis using TextBlob
                from textblob import TextBlob
                
                # Sample sentence
                sentence = "I love Natural Language Processing!"
                blob = TextBlob(sentence)
                
                # Analyzing sentiment
                print("Sentiment polarity:", blob.sentiment.polarity)
                print("Sentiment subjectivity:", blob.sentiment.subjectivity)</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 6: Word Embeddings</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Understand how word embeddings represent words in high-dimensional space.</p>
                                <p><strong>Scenario:</strong> Write a Python script using the `gensim` library to load pre-trained word embeddings and find similar words.</p>
                                <p><strong>Key Concepts:</strong> Word embeddings, word2vec, vector representation.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution"># Word embeddings using gensim
                import gensim.downloader as api
                
                # Load pre-trained word embeddings
                model = api.load("glove-wiki-gigaword-50")
                
                # Find similar words
                word = "king"
                similar_words = model.most_similar(word)
                print(f"Words similar to '{word}':", similar_words)</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 7: Machine Translation</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Learn how to perform machine translation using NLP tools.</p>
                                <p><strong>Scenario:</strong> Write a Python script using `transformers` to translate text from English to French.</p>
                                <p><strong>Key Concepts:</strong> Machine translation, transformer models, sequence-to-sequence learning.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution"># Machine translation using transformers
                from transformers import pipeline
                
                # Load translation pipeline
                translator = pipeline("translation_en_to_fr")
                
                # Translate text
                text = "I love learning NLP."
                translation = translator(text)
                print("Translation:", translation[0]['translation_text'])</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 8: Text Summarization</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Learn how to generate summaries from long text.</p>
                                <p><strong>Scenario:</strong> Write a Python script using the `transformers` library to summarize a paragraph.</p>
                                <p><strong>Key Concepts:</strong> Text summarization, extractive summarization, abstractive summarization.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution"># Text summarization using transformers
                from transformers import pipeline
                
                # Load summarization pipeline
                summarizer = pipeline("summarization")
                
                # Sample paragraph
                text = '''Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language. The ultimate goal of NLP is to enable computers to understand, interpret, and respond to human language in a valuable way.'''
                
                # Summarizing text
                summary = summarizer(text)
                print("Summary:", summary[0]['summary_text'])</pre>
                            </div>
                        </li>
                
                    </ul>
                </div>
                
                <!-- Modular Design -->

                <div id="case-study3" class="main-content3">
                    <h2 id="feature-engineering-section">Feature Engineering</h2><br>
                    <ul class="chapter-list">
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 1: Introduction to Feature Engineering</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Understand the concept of feature engineering in machine learning.</p>
                                <p><strong>Scenario:</strong> Write a Python script that demonstrates basic feature engineering techniques, including encoding categorical variables and normalizing numerical features.</p>
                                <p><strong>Key Concepts:</strong> Feature selection, encoding, normalization, data preprocessing.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">import pandas as pd
                from sklearn.preprocessing import OneHotEncoder, StandardScaler
                
                # Sample data
                data = pd.DataFrame({
                    'category': ['A', 'B', 'A', 'C'],
                    'value': [10, 20, 15, 25]
                })
                
                # One-hot encoding categorical variables
                encoder = OneHotEncoder(sparse=False)
                encoded_categories = encoder.fit_transform(data[['category']])
                encoded_df = pd.DataFrame(encoded_categories, columns=encoder.get_feature_names_out(['category']))
                
                # Normalizing numerical features
                scaler = StandardScaler()
                data['value_normalized'] = scaler.fit_transform(data[['value']])
                
                # Final dataset
                final_df = pd.concat([data, encoded_df], axis=1)
                print(final_df)</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 2: Feature Scaling Techniques</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Learn how to apply different feature scaling techniques in Python.</p>
                                <p><strong>Scenario:</strong> Implement Min-Max Scaling and Standardization on a dataset.</p>
                                <p><strong>Key Concepts:</strong> Min-Max Scaling, Standardization, scaling methods.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">from sklearn.preprocessing import MinMaxScaler, StandardScaler
                
                # Sample data
                data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]
                
                # Min-Max Scaling
                min_max_scaler = MinMaxScaler()
                min_max_scaled_data = min_max_scaler.fit_transform(data)
                
                # Standardization
                standard_scaler = StandardScaler()
                standardized_data = standard_scaler.fit_transform(data)
                
                print("Min-Max Scaled Data:\n", min_max_scaled_data)
                print("Standardized Data:\n", standardized_data)</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 3: Feature Creation from Existing Features</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Understand how to create new features from existing ones.</p>
                                <p><strong>Scenario:</strong> Write a Python script that generates new features based on existing numerical data, such as polynomial features and interaction terms.</p>
                                <p><strong>Key Concepts:</strong> Feature creation, polynomial features, interaction terms.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">from sklearn.preprocessing import PolynomialFeatures
                import numpy as np
                
                # Sample data
                X = np.array([[1, 2], [3, 4], [5, 6]])
                
                # Creating polynomial features
                poly = PolynomialFeatures(degree=2)
                poly_features = poly.fit_transform(X)
                
                print("Original Features:\n", X)
                print("Polynomial Features:\n", poly_features)</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 4: Handling Missing Values</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Learn techniques to handle missing values in a dataset.</p>
                                <p><strong>Scenario:</strong> Write a Python script that demonstrates how to fill in missing values and drop rows with missing data.</p>
                                <p><strong>Key Concepts:</strong> Missing data, imputation, dropping missing values.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution"># Sample data with missing values
                import numpy as np
                import pandas as pd
                
                data = pd.DataFrame({
                    'A': [1, 2, np.nan, 4],
                    'B': [np.nan, 2, 3, 4]
                })
                
                # Filling missing values with the mean
                data_filled = data.fillna(data.mean())
                
                # Dropping rows with missing values
                data_dropped = data.dropna()
                
                print("Filled Data:\n", data_filled)
                print("Dropped Data:\n", data_dropped)</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 5: Encoding Categorical Features</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Understand different techniques for encoding categorical variables.</p>
                                <p><strong>Scenario:</strong> Write a Python script that demonstrates label encoding and one-hot encoding.</p>
                                <p><strong>Key Concepts:</strong> Label encoding, one-hot encoding, categorical variables.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">from sklearn.preprocessing import LabelEncoder
                import pandas as pd
                
                # Sample data
                data = pd.DataFrame({
                    'color': ['red', 'blue', 'green', 'blue'],
                })
                
                # Label encoding
                label_encoder = LabelEncoder()
                data['color_encoded'] = label_encoder.fit_transform(data['color'])
                
                print("Original Data:\n", data)</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 6: Feature Selection Techniques</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Learn techniques for selecting the most relevant features in a dataset.</p>
                                <p><strong>Scenario:</strong> Implement feature selection techniques using Python libraries.</p>
                                <p><strong>Key Concepts:</strong> Feature selection, relevance, dimensionality reduction.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">from sklearn.datasets import load_iris
                from sklearn.feature_selection import SelectKBest, f_classif
                
                # Load dataset
                iris = load_iris()
                X, y = iris.data, iris.target
                
                # Select top 2 features
                selector = SelectKBest(score_func=f_classif, k=2)
                X_selected = selector.fit_transform(X, y)
                
                print("Original Features:\n", X)
                print("Selected Features:\n", X_selected)</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 7: Working with Time-Series Features</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Understand how to engineer features for time-series data.</p>
                                <p><strong>Scenario:</strong> Write a Python script that demonstrates how to create lagged features and rolling statistics.</p>
                                <p><strong>Key Concepts:</strong> Time-series data, lagged features, rolling statistics.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution"># Sample time-series data
                import pandas as pd
                
                # Create time-series data
                date_range = pd.date_range(start='2023-01-01', periods=5)
                data = pd.DataFrame({'date': date_range, 'value': [1, 2, 3, 4, 5]})
                
                # Set date as index
                data.set_index('date', inplace=True)
                
                # Create lagged features
                data['lagged_value'] = data['value'].shift(1)
                
                # Create rolling statistics
                data['rolling_mean'] = data['value'].rolling(window=2).mean()
                
                print(data)</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 8: Feature Engineering for Models</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Learn how to engineer features specifically for different machine learning models.</p>
                                <p><strong>Scenario:</strong> Write a Python script that demonstrates how to prepare features for regression and classification models.</p>
                                <p><strong>Key Concepts:</strong> Feature engineering for models, regression features, classification features.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution"># Sample data for regression and classification
                import pandas as pd
                
                # Regression features
                regression_data = pd.DataFrame({
                    'feature1': [1, 2, 3],
                    'feature2': [4, 5, 6],
                    'target': [7, 8, 9]
                })
                
                # Classification features
                classification_data = pd.DataFrame({
                    'feature1': [1, 2, 3],
                    'feature2': [1, 0, 1],
                    'target': [0, 1, 0]
                })
                
                print("Regression Data:\n", regression_data)
                print("Classification Data:\n", classification_data)</pre>
                            </div>
                        </li>
                    </ul>
                </div>


                <!-- Text Files -->

                <div id="case-study3" class="main-content3">
                    <h2 id="text-files-section">Model Evaluation and Validation</h2><br>
                    <ul class="chapter-list">
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 1: Evaluating Model Performance</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Learn how to evaluate the performance of machine learning models.</p>
                                <p><strong>Scenario:</strong> Write a Python script to compute the accuracy of a classification model using sklearn.</p>
                                <p><strong>Key Concepts:</strong> Model performance metrics, accuracy, sklearn.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution"># Evaluating model performance
                from sklearn.metrics import accuracy_score
                
                # Example true and predicted labels
                y_true = [0, 1, 0, 1]
                y_pred = [0, 1, 1, 0]
                
                # Calculate accuracy
                accuracy = accuracy_score(y_true, y_pred)
                print(f'Accuracy: {accuracy:.2f}')</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 2: Cross-Validation</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Understand how to perform cross-validation for better model evaluation.</p>
                                <p><strong>Scenario:</strong> Write a Python script that uses k-fold cross-validation on a dataset.</p>
                                <p><strong>Key Concepts:</strong> Cross-validation, k-fold, model evaluation.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution"># Performing k-fold cross-validation
                from sklearn.model_selection import cross_val_score
                from sklearn.datasets import load_iris
                from sklearn.ensemble import RandomForestClassifier
                
                # Load dataset
                data = load_iris()
                X, y = data.data, data.target
                
                # Define model
                model = RandomForestClassifier()
                
                # Perform cross-validation
                scores = cross_val_score(model, X, y, cv=5)
                print(f'Cross-validation scores: {scores}')</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 3: Hyperparameter Tuning</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Learn how to tune hyperparameters for better model performance.</p>
                                <p><strong>Scenario:</strong> Write a Python script that performs GridSearchCV to find the best hyperparameters.</p>
                                <p><strong>Key Concepts:</strong> Hyperparameter tuning, GridSearchCV, sklearn.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution"># Hyperparameter tuning using GridSearchCV
                from sklearn.model_selection import GridSearchCV
                
                # Define model and parameters
                model = RandomForestClassifier()
                param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20]}
                
                # Perform Grid Search
                grid_search = GridSearchCV(model, param_grid, cv=5)
                grid_search.fit(X, y)
                
                print(f'Best parameters: {grid_search.best_params_}')</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 4: Confusion Matrix</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Understand how to visualize model performance using a confusion matrix.</p>
                                <p><strong>Scenario:</strong> Write a Python script to compute and plot the confusion matrix for a classification model.</p>
                                <p><strong>Key Concepts:</strong> Confusion matrix, visualization, matplotlib.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution"># Plotting confusion matrix
                from sklearn.metrics import confusion_matrix
                import matplotlib.pyplot as plt
                import seaborn as sns
                
                # Example true and predicted labels
                y_true = [0, 1, 0, 1]
                y_pred = [0, 1, 1, 0]
                
                # Compute confusion matrix
                cm = confusion_matrix(y_true, y_pred)
                
                # Plot confusion matrix
                sns.heatmap(cm, annot=True, fmt='d')
                plt.xlabel('Predicted')
                plt.ylabel('True')
                plt.title('Confusion Matrix')
                plt.show()</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 5: ROC Curve</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Learn how to plot the ROC curve and calculate the AUC for model evaluation.</p>
                                <p><strong>Scenario:</strong> Write a Python script that calculates the ROC curve and AUC for a binary classifier.</p>
                                <p><strong>Key Concepts:</strong> ROC curve, AUC, binary classification.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution"># Plotting ROC curve
                from sklearn.metrics import roc_curve, auc
                import matplotlib.pyplot as plt
                
                # Example true labels and predicted probabilities
                y_true = [0, 1, 0, 1]
                y_scores = [0.1, 0.9, 0.4, 0.8]
                
                # Calculate ROC curve and AUC
                fpr, tpr, thresholds = roc_curve(y_true, y_scores)
                roc_auc = auc(fpr, tpr)
                
                # Plot ROC curve
                plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')
                plt.plot([0, 1], [0, 1], 'k--')
                plt.xlabel('False Positive Rate')
                plt.ylabel('True Positive Rate')
                plt.title('Receiver Operating Characteristic (ROC) Curve')
                plt.legend(loc='best')
                plt.show()</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 6: Feature Importance</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Understand how to interpret feature importance in model evaluation.</p>
                                <p><strong>Scenario:</strong> Write a Python script that calculates and visualizes feature importance for a tree-based model.</p>
                                <p><strong>Key Concepts:</strong> Feature importance, interpretation, visualization.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution"># Feature importance visualization
                importances = model.feature_importances_
                features = data.feature_names
                
                # Create a bar plot of feature importance
                plt.barh(features, importances)
                plt.xlabel('Feature Importance')
                plt.title('Feature Importance for Random Forest Model')
                plt.show()</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 7: Overfitting and Underfitting</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Learn to identify overfitting and underfitting in models.</p>
                                <p><strong>Scenario:</strong> Write a Python script that visualizes training and validation accuracy over epochs.</p>
                                <p><strong>Key Concepts:</strong> Overfitting, underfitting, training vs validation performance.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution"># Visualizing training and validation performance
                history = model.fit(X_train, y_train)
                
                plt.plot(history.history['accuracy'], label='Train Accuracy')
                plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
                plt.xlabel('Epochs')
                plt.ylabel('Accuracy')
                plt.title('Training and Validation Accuracy')
                plt.legend()
                plt.show()</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 8: Model Selection</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Understand how to select the best model among multiple candidates.</p>
                                <p><strong>Scenario:</strong> Write a Python script that compares different models based on cross-validation scores.</p>
                                <p><strong>Key Concepts:</strong> Model selection, cross-validation, comparison metrics.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution"># Comparing different models
                from sklearn.linear_model import LogisticRegression
                from sklearn.svm import SVC
                
                models = [LogisticRegression(), SVC()]
                for model in models:
                    scores = cross_val_score(model, X, y, cv=5)
                    print(f'{model.__class__.__name__} Cross-validation scores: {scores}')</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 9: Ensemble Methods</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Learn how ensemble methods improve model performance.</p>
                                <p><strong>Scenario:</strong> Write a Python script to implement bagging or boosting on a dataset.</p>
                                <p><strong>Key Concepts:</strong> Ensemble methods, bagging, boosting.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution"># Implementing bagging with Random Forest
                from sklearn.ensemble import RandomForestClassifier
                
                model = RandomForestClassifier(n_estimators=100)
                model.fit(X_train, y_train)
                print('Bagging model trained successfully.')</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 10: Saving and Loading Models</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Understand how to save and load trained models for future use.</p>
                                <p><strong>Scenario:</strong> Write a Python script that saves a trained model to disk and loads it back.</p>
                                <p><strong>Key Concepts:</strong> Model persistence, joblib, pickle.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution"># Saving and loading a model
                import joblib
                
                # Save model
                joblib.dump(model, 'trained_model.pkl')
                
                # Load model
                loaded_model = joblib.load('trained_model.pkl')
                print('Model loaded successfully.')</pre>
                            </div>
                        </li>
                    </ul>
                </div>
                

                <!-- disct -->

                <div id="case-study3" class="main-content3">
                    <h2 id="ensemble-learning-section">Ensemble Learning</h2><br>
                    <ul class="chapter-list">
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 1: Introduction to Ensemble Learning</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Understand the concept of ensemble learning and its importance in improving model performance.</p>
                                <p><strong>Scenario:</strong> Write a brief explanation of what ensemble learning is and its common applications.</p>
                                <p><strong>Key Concepts:</strong> Ensemble methods, model accuracy, bias-variance tradeoff.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution"># Ensemble learning combines predictions from multiple models to improve accuracy
                                # Common applications include classification and regression tasks in machine learning.</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 2: Bagging Method</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Learn how bagging (Bootstrap Aggregating) works to reduce variance.</p>
                                <p><strong>Scenario:</strong> Implement a bagging ensemble method using Random Forest on a dataset.</p>
                                <p><strong>Key Concepts:</strong> Bagging, Random Forest, reducing overfitting.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">from sklearn.ensemble import RandomForestClassifier
                from sklearn.datasets import load_iris
                from sklearn.model_selection import train_test_split
                
                # Load dataset and split
                data = load_iris()
                X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2)
                
                # Creating and training Random Forest model
                model = RandomForestClassifier(n_estimators=100)
                model.fit(X_train, y_train)
                accuracy = model.score(X_test, y_test)
                print('Bagging model accuracy:', accuracy)</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 3: Boosting Method</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Understand how boosting works to improve model performance by focusing on misclassified instances.</p>
                                <p><strong>Scenario:</strong> Implement a boosting algorithm like AdaBoost on a dataset.</p>
                                <p><strong>Key Concepts:</strong> Boosting, AdaBoost, focus on difficult cases.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">from sklearn.ensemble import AdaBoostClassifier
                from sklearn.tree import DecisionTreeClassifier
                
                # Creating and training AdaBoost model
                base_model = DecisionTreeClassifier(max_depth=1)
                boost_model = AdaBoostClassifier(base_estimator=base_model, n_estimators=50)
                boost_model.fit(X_train, y_train)
                boost_accuracy = boost_model.score(X_test, y_test)
                print('Boosting model accuracy:', boost_accuracy)</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 4: Stacking Method</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Learn about stacking and how it combines different models to improve predictions.</p>
                                <p><strong>Scenario:</strong> Implement a stacking ensemble method using scikit-learn.</p>
                                <p><strong>Key Concepts:</strong> Stacking, meta-learners, model diversity.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">from sklearn.ensemble import StackingClassifier
                from sklearn.linear_model import LogisticRegression
                from sklearn.svm import SVC
                
                # Creating base models and stacking classifier
                estimators = [('svr', SVC()), ('lr', LogisticRegression())]
                stacking_model = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())
                stacking_model.fit(X_train, y_train)
                stacking_accuracy = stacking_model.score(X_test, y_test)
                print('Stacking model accuracy:', stacking_accuracy)</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 5: Model Comparison</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Compare the performance of different ensemble methods.</p>
                                <p><strong>Scenario:</strong> Evaluate and compare the accuracy of bagging, boosting, and stacking methods on the same dataset.</p>
                                <p><strong>Key Concepts:</strong> Model evaluation, accuracy comparison, ensemble performance.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution"># Comparing different ensemble methods
                print('Bagging accuracy:', accuracy)
                print('Boosting accuracy:', boost_accuracy)
                print('Stacking accuracy:', stacking_accuracy)</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 6: Hyperparameter Tuning in Ensemble Models</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Learn how to perform hyperparameter tuning for ensemble models.</p>
                                <p><strong>Scenario:</strong> Use GridSearchCV to find the best hyperparameters for a Random Forest model.</p>
                                <p><strong>Key Concepts:</strong> Hyperparameter tuning, GridSearchCV, model optimization.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">from sklearn.model_selection import GridSearchCV
                
                # Setting parameters for tuning
                param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20]}
                grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)
                grid_search.fit(X_train, y_train)
                print('Best parameters:', grid_search.best_params_)</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 7: Using Ensemble Models in Real-World Applications</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Explore real-world applications of ensemble methods.</p>
                                <p><strong>Scenario:</strong> Write a short report on how ensemble learning is used in industries such as finance, healthcare, and marketing.</p>
                                <p><strong>Key Concepts:</strong> Real-world applications, ensemble learning impact.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution"># Ensemble learning is widely used for credit scoring, fraud detection, patient diagnosis,
                                # and customer segmentation in various industries.</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 8: Ensemble Learning with Neural Networks</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Understand how ensemble learning applies to neural networks.</p>
                                <p><strong>Scenario:</strong> Implement an ensemble of neural networks using Keras and evaluate their performance.</p>
                                <p><strong>Key Concepts:</strong> Neural network ensembles, model diversity, performance improvement.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">from keras.models import Sequential
                from keras.layers import Dense
                import numpy as np
                
                # Creating and training multiple neural networks
                def create_model():
                    model = Sequential()
                    model.add(Dense(10, input_dim=4, activation='relu'))
                    model.add(Dense(3, activation='softmax'))
                    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
                    return model
                
                models = [create_model() for _ in range(5)]
                for model in models:
                    model.fit(X_train, y_train, epochs=10, verbose=0)
                
                # Evaluating ensemble performance would be more complex
                print('Neural network ensemble trained successfully.')</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 9: Visualizing Ensemble Model Performance</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Learn how to visualize the performance of ensemble models.</p>
                                <p><strong>Scenario:</strong> Create plots to compare the accuracy and loss of different ensemble methods.</p>
                                <p><strong>Key Concepts:</strong> Model visualization, performance metrics.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">import matplotlib.pyplot as plt
                
                # Sample data for visualization (accuracy for simplicity)
                accuracies = [accuracy, boost_accuracy, stacking_accuracy]
                methods = ['Bagging', 'Boosting', 'Stacking']
                
                plt.bar(methods, accuracies)
                plt.ylabel('Accuracy')
                plt.title('Ensemble Method Comparison')
                plt.show()</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 10: Challenges in Ensemble Learning</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Identify and discuss the challenges faced in implementing ensemble learning.</p>
                                <p><strong>Scenario:</strong> Write an essay on common challenges like model interpretability, computational cost, and data dependency.</p>
                                <p><strong>Key Concepts:</strong> Challenges in ensemble learning, model complexity.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution"># Challenges in ensemble learning include interpretability, higher computational costs,
                                # and the dependency on the quality and quantity of data used for training.</pre>
                            </div>
                        </li>
                
                    </ul>
                </div>

                <!-- oops -->

                <div id="case-study3" class="main-content3">
                    <h2 id="dimensionality-reduction-section">Dimensionality Reduction</h2><br>
                    <ul class="chapter-list">
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 1: Introduction to Dimensionality Reduction</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Understand the concept and importance of dimensionality reduction.</p>
                                <p><strong>Scenario:</strong> Write a brief explanation of what dimensionality reduction is and its applications in machine learning.</p>
                                <p><strong>Key Concepts:</strong> High dimensionality, overfitting, feature extraction.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution"># Dimensionality reduction techniques help in reducing the number of features
                                # in a dataset while retaining essential information, thus improving model performance.</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 2: Principal Component Analysis (PCA)</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Learn how to implement PCA for dimensionality reduction.</p>
                                <p><strong>Scenario:</strong> Use PCA on a dataset to reduce its dimensions and visualize the results.</p>
                                <p><strong>Key Concepts:</strong> Eigenvalues, eigenvectors, variance preservation.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">from sklearn.decomposition import PCA
                from sklearn.datasets import load_iris
                import matplotlib.pyplot as plt
                
                # Load dataset
                data = load_iris()
                X = data.data
                
                # Apply PCA to reduce to 2 dimensions
                pca = PCA(n_components=2)
                X_reduced = pca.fit_transform(X)
                
                # Visualizing the reduced data
                plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=data.target)
                plt.xlabel('Principal Component 1')
                plt.ylabel('Principal Component 2')
                plt.title('PCA Result')
                plt.show()</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 3: t-Distributed Stochastic Neighbor Embedding (t-SNE)</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Understand how to use t-SNE for visualizing high-dimensional data.</p>
                                <p><strong>Scenario:</strong> Apply t-SNE on a dataset and visualize the clusters.</p>
                                <p><strong>Key Concepts:</strong> Non-linear dimensionality reduction, local vs global structure.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">from sklearn.manifold import TSNE
                
                # Apply t-SNE to reduce dimensions
                tsne = TSNE(n_components=2)
                X_tsne = tsne.fit_transform(X)
                
                # Visualizing the t-SNE result
                plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=data.target)
                plt.xlabel('t-SNE Component 1')
                plt.ylabel('t-SNE Component 2')
                plt.title('t-SNE Result')
                plt.show()</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 4: Linear Discriminant Analysis (LDA)</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Learn how to apply LDA for dimensionality reduction in supervised learning.</p>
                                <p><strong>Scenario:</strong> Implement LDA on a labeled dataset and visualize the results.</p>
                                <p><strong>Key Concepts:</strong> Class separability, supervised dimensionality reduction.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
                
                # Apply LDA to reduce dimensions
                lda = LinearDiscriminantAnalysis(n_components=2)
                X_lda = lda.fit_transform(X, data.target)
                
                # Visualizing the LDA result
                plt.scatter(X_lda[:, 0], X_lda[:, 1], c=data.target)
                plt.xlabel('LDA Component 1')
                plt.ylabel('LDA Component 2')
                plt.title('LDA Result')
                plt.show()</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 5: Feature Selection vs. Dimensionality Reduction</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Differentiate between feature selection and dimensionality reduction.</p>
                                <p><strong>Scenario:</strong> Write a short report comparing the two techniques.</p>
                                <p><strong>Key Concepts:</strong> Feature selection, feature extraction, use cases.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution"># Feature selection involves choosing a subset of original features,
                                # while dimensionality reduction transforms the features into a lower-dimensional space.</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 6: Challenges in Dimensionality Reduction</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Identify challenges in implementing dimensionality reduction techniques.</p>
                                <p><strong>Scenario:</strong> Discuss issues such as loss of information and computational cost.</p>
                                <p><strong>Key Concepts:</strong> Information loss, computational complexity, parameter tuning.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution"># Challenges include the risk of losing important information
                                # and the computational cost of certain methods like t-SNE.</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 7: Using Dimensionality Reduction in Machine Learning Pipelines</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Learn how to incorporate dimensionality reduction in machine learning workflows.</p>
                                <p><strong>Scenario:</strong> Build a pipeline that includes PCA and a classifier.</p>
                                <p><strong>Key Concepts:</strong> Scikit-learn pipelines, model integration.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">from sklearn.pipeline import Pipeline
                from sklearn.ensemble import RandomForestClassifier
                
                # Creating a pipeline with PCA and Random Forest
                pipeline = Pipeline([
                    ('pca', PCA(n_components=2)),
                    ('classifier', RandomForestClassifier())
                ])
                
                # Fitting the pipeline on the dataset
                pipeline.fit(X, data.target)
                print("Pipeline trained with PCA and classifier.")</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 8: Evaluating Dimensionality Reduction Techniques</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Understand how to evaluate the effectiveness of dimensionality reduction techniques.</p>
                                <p><strong>Scenario:</strong> Compare the classification accuracy with and without dimensionality reduction.</p>
                                <p><strong>Key Concepts:</strong> Model evaluation, accuracy comparison.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution"># Evaluating the performance of models before and after dimensionality reduction
                # Example code to compare accuracy can be implemented based on chosen classifiers.</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 9: Dimensionality Reduction in Image Processing</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Explore the application of dimensionality reduction in image processing.</p>
                                <p><strong>Scenario:</strong> Use PCA to compress an image dataset.</p>
                                <p><strong>Key Concepts:</strong> Image compression, PCA in image processing.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">from sklearn.datasets import load_sample_image
                import numpy as np
                
                # Load sample image and reshape it
                china = load_sample_image("china.jpg")
                data = np.array(china).reshape(-1, 3)
                
                # Apply PCA to reduce dimensions for image compression
                pca = PCA(n_components=50)
                compressed_data = pca.fit_transform(data)
                
                # The compressed_data can now be reconstructed into an image.</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 10: Future Directions in Dimensionality Reduction</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Discuss emerging trends and future research in dimensionality reduction.</p>
                                <p><strong>Scenario:</strong> Write an essay on novel approaches to dimensionality reduction.</p>
                                <p><strong>Key Concepts:</strong> Emerging techniques, deep learning, manifold learning.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution"># Future research may focus on developing more efficient algorithms
                                # and exploring deep learning approaches for better dimensionality reduction.</pre>
                            </div>
                        </li>
                
                    </ul>
                </div>

                <!-- recursion -->

                <div id="case-study3" class="main-content3">
                    <h2 id="time-series-analysis-section">Time Series Analysis and Forecasting</h2><br>
                    <ul class="chapter-list">
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 1: Introduction to Time Series</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Understand the basics of time series data.</p>
                                <p><strong>Scenario:</strong> Write a brief explanation of what a time series is and its applications.</p>
                                <p><strong>Key Concepts:</strong> Time series data, temporal ordering, seasonality.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution"># Time series data is a sequence of data points collected or recorded at specific time intervals.
                                # Common applications include stock prices, weather data, and economic indicators.</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 2: Time Series Decomposition</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Learn how to decompose a time series into its components.</p>
                                <p><strong>Scenario:</strong> Use seasonal decomposition to separate trend, seasonality, and noise in a time series dataset.</p>
                                <p><strong>Key Concepts:</strong> Trend, seasonality, noise, additive and multiplicative decomposition.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">import pandas as pd
                import statsmodels.api as sm
                
                # Load time series data
                data = pd.read_csv('time_series_data.csv', parse_dates=True, index_col='date')
                decomposition = sm.tsa.seasonal_decompose(data['value'], model='additive')
                decomposition.plot()
                plt.show()</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 3: Autocorrelation and Partial Autocorrelation</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Understand autocorrelation and its significance in time series analysis.</p>
                                <p><strong>Scenario:</strong> Calculate and visualize the autocorrelation and partial autocorrelation of a time series.</p>
                                <p><strong>Key Concepts:</strong> Autocorrelation function (ACF), partial autocorrelation function (PACF).</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
                
                # Plot ACF and PACF
                plot_acf(data['value'])
                plt.title('Autocorrelation Function')
                plt.show()
                
                plot_pacf(data['value'])
                plt.title('Partial Autocorrelation Function')
                plt.show()</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 4: ARIMA Model</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Learn how to implement an ARIMA model for time series forecasting.</p>
                                <p><strong>Scenario:</strong> Fit an ARIMA model to a time series dataset and make forecasts.</p>
                                <p><strong>Key Concepts:</strong> ARIMA model, parameters (p, d, q), forecasting.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">from statsmodels.tsa.arima.model import ARIMA
                
                # Fit ARIMA model
                model = ARIMA(data['value'], order=(1, 1, 1))
                model_fit = model.fit()
                
                # Make forecast
                forecast = model_fit.forecast(steps=5)
                print("Forecasted values:", forecast)</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 5: Seasonal ARIMA (SARIMA)</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Understand and implement SARIMA for seasonal time series data.</p>
                                <p><strong>Scenario:</strong> Fit a SARIMA model to a seasonal time series dataset.</p>
                                <p><strong>Key Concepts:</strong> Seasonal decomposition, SARIMA parameters, seasonal effects.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">from statsmodels.tsa.statespace.sarimax import SARIMAX
                
                # Fit SARIMA model
                seasonal_order = (1, 1, 1, 12)  # Example seasonal order
                model = SARIMAX(data['value'], order=(1, 1, 1), seasonal_order=seasonal_order)
                model_fit = model.fit()
                
                # Make forecast
                forecast = model_fit.forecast(steps=5)
                print("Forecasted seasonal values:", forecast)</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 6: Exponential Smoothing</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Learn about exponential smoothing methods for forecasting.</p>
                                <p><strong>Scenario:</strong> Apply simple and Holt-Winters exponential smoothing to a time series.</p>
                                <p><strong>Key Concepts:</strong> Exponential smoothing, trend, seasonality.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">from statsmodels.tsa.holtwinters import SimpleExpSmoothing, Holt, ExponentialSmoothing
                
                # Simple Exponential Smoothing
                model = SimpleExpSmoothing(data['value'])
                model_fit = model.fit()
                forecast_simple = model_fit.forecast(steps=5)
                print("Simple Exponential Smoothing forecast:", forecast_simple)
                
                # Holt-Winters Exponential Smoothing
                model_hw = ExponentialSmoothing(data['value'], seasonal='add', seasonal_periods=12)
                model_fit_hw = model_hw.fit()
                forecast_hw = model_fit_hw.forecast(steps=5)
                print("Holt-Winters forecast:", forecast_hw)</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 7: Model Evaluation Metrics</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Understand how to evaluate forecasting models.</p>
                                <p><strong>Scenario:</strong> Calculate RMSE and MAE for model performance assessment.</p>
                                <p><strong>Key Concepts:</strong> RMSE, MAE, model evaluation.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution">from sklearn.metrics import mean_squared_error, mean_absolute_error
                import numpy as np
                
                # Calculate RMSE and MAE
                rmse = np.sqrt(mean_squared_error(actual_values, forecasted_values))
                mae = mean_absolute_error(actual_values, forecasted_values)
                print("RMSE:", rmse)
                print("MAE:", mae)</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 8: Cross-Validation for Time Series</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Implement cross-validation for time series forecasting.</p>
                                <p><strong>Scenario:</strong> Use time series cross-validation techniques to assess model performance.</p>
                                <p><strong>Key Concepts:</strong> Time series cross-validation, rolling forecast, expanding window.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution"># Example implementation of time series cross-validation can be done
                                # using techniques like rolling forecasts or expanding windows to validate model performance.</pre>
                            </div>
                        </li>
                
                        <li class="chapter-item">
                            <div class="chapter-header">
                                <span>Case Study 9: Anomaly Detection in Time Series</span>
                                <span class="chapter-toggle">▼</span>
                            </div>
                            <div class="chapter-content">
                                <p><strong>Objective:</strong> Learn how to detect anomalies in time series data.</p>
                                <p><strong>Scenario:</strong> Apply methods to identify outliers in a time series dataset.</p>
                                <p><strong>Key Concepts:</strong> Anomaly detection, statistical tests, thresholding.</p>
                                <button onclick="toggleSolution(this)">Show Solution</button>
                                <pre class="solution"># Example techniques include using statistical thresholds,
                                # Z-scores, or machine learning approaches to identify anomalies in time series data.</pre>
                            </div>
                        </li>
                    </ul>
                </div>



                        <!-- Read More Button
        <div id="read-more-container" style="text-align: center; margin: 20px 0; display: none;">
            <button id="read-more-btn" onclick="toggleReadMore()">Read More ▼</button>
        </div>
    </div> -->


                        <script>
                            function toggleSolution(element) {
                                const solution = element.nextElementSibling;
                                const expanded = element.getAttribute('aria-expanded') === 'true';
                                element.setAttribute('aria-expanded', !expanded);
                                solution.style.display = expanded ? 'none' : 'block';

                                // Apply styles to make it look like a code solution
                                solution.style.backgroundColor = expanded ? '' : '#1e1e1e'; // Dark background similar to VS Code
                                solution.style.color = expanded ? '' : '#d4d4d4'; // Light text color
                                solution.style.padding = expanded ? '' : '10px'; // Padding for a code block feel
                                solution.style.borderRadius = expanded ? '' : '5px'; // Rounded corners
                                solution.style.fontFamily = expanded ? '' : 'Consolas, "Courier New", monospace'; // Monospace font
                                solution.style.border = expanded ? '' : '1px solid #333'; // Border to give it a distinct look
                            }

                            function checkReadMoreVisibility() {
                                const chapterItems = document.querySelectorAll('.chapter-item');
                                const readMoreContainer = document.getElementById('read-more-container');

                                if (chapterItems.length > 5) {
                                    readMoreContainer.style.display = 'block';
                                }
                            }

                            function toggleReadMore() {
                                const hiddenChapters = document.getElementById('hidden-chapters');
                                const readMoreBtn = document.getElementById('read-more-btn');

                                if (hiddenChapters.style.display === 'none') {
                                    hiddenChapters.style.display = 'block';
                                    readMoreBtn.innerText = 'Read Less ▲';
                                } else {
                                    hiddenChapters.style.display = 'none';
                                    readMoreBtn.innerText = 'Read More ▼';
                                }
                            }

                            document.addEventListener('DOMContentLoaded', () => {
                                // Hide all solutions by default
                                const solutions = document.querySelectorAll('.solution');
                                solutions.forEach(solution => {
                                    solution.style.display = 'none'; // Hide each solution
                                });

                                checkReadMoreVisibility();
                            });

                        </script>

















                        <div id="faqs" class="main-content">
                            <h2>Frequently Asked Questions</h2>
                            <div class="faq-item">
                                <div class="faq-question">Who is this course for?</div>
                                <div class="faq-answer">
                                    <p>This course is designed for software engineers, tech leads, and architects who
                                        want to deepen
                                        their understanding of system design. It's also excellent preparation for those
                                        facing
                                        system design interviews.</p>
                                </div>
                            </div>
                            <div class="faq-item">
                                <div class="faq-question">What prerequisites are needed?</div>
                                <div class="faq-answer">
                                    <p>A solid understanding of basic programming concepts and some experience with web
                                        development
                                        is recommended. Familiarity with distributed systems is helpful but not
                                        required.</p>
                                </div>
                            </div>
                            <div class="faq-item">
                                <div class="faq-question">How long does the course take to complete?</div>
                                <div class="faq-answer">
                                    <p>The course is self-paced, but typically takes 8-10 weeks to complete if you
                                        dedicate 5-7
                                        hours per week. You'll have lifetime access to the content, so you can learn at
                                        your own
                                        pace.</p>
                                </div>
                            </div>
                        </div>
                </div>

                <!-- Premium Access Overlay -->
                <div class="premium-overlay" id="premiumOverlay">
                    <div class="premium-content">
                        <h2 class="premium-title">Unlock Premium Access</h2>
                        <p class="premium-price">$199</p>
                        <ul class="premium-features">
                            <li><i class="fa fa-check-circle"></i> Full access to all course modules</li>
                            <li><i class="fa fa-briefcase"></i> Exclusive case studies and projects</li>
                            <li><i class="fa fa-user-tie"></i> 1-on-1 mentoring sessions</li>
                            <li><i class="fa fa-certificate"></i> Certificate of completion</li>
                            <li><i class="fa fa-refresh"></i> Lifetime updates</li>
                        </ul>
                        <button class="cta-button">Enroll Now</button>
                        <button class="close-button" id="closeOverlay">Close</button>
                    </div>
                </div>

            </div>

            <!-- <script>
        function toggleSolution(element) {
            const solution = element.nextElementSibling;
            const expanded = element.getAttribute('aria-expanded') === 'true' || false;
            element.setAttribute('aria-expanded', !expanded);
            solution.style.display = expanded ? 'none' : 'block';
    
            // Apply styles to make it look like a code solution
            solution.style.backgroundColor = expanded ? '' : '#1e1e1e'; // Dark background similar to VS Code
            solution.style.color = expanded ? '' : '#d4d4d4'; // Light text color
            solution.style.padding = expanded ? '' : '10px'; // Padding for a code block feel
            solution.style.borderRadius = expanded ? '' : '5px'; // Rounded corners
            solution.style.fontFamily = expanded ? '' : 'Consolas, "Courier New", monospace'; // Monospace font
            solution.style.border = expanded ? '' : '1px solid #333'; // Border to give it a distinct look
        }
    
        function checkReadMoreVisibility() {
            const chapterItems = document.querySelectorAll('.chapter-item');
            const readMoreContainer = document.getElementById('read-more-container');
    
            if (chapterItems.length > 5) {
                readMoreContainer.style.display = 'block';
            }
        }
    
        function toggleReadMore() {
            var hiddenChapters = document.getElementById('hidden-chapters');
            var readMoreBtn = document.getElementById('read-more-btn');
    
            if (hiddenChapters.style.display === 'none') {
                hiddenChapters.style.display = 'block';
                readMoreBtn.textContent = 'Show Less ▲';
            } else {
                hiddenChapters.style.display = 'none';
                readMoreBtn.textContent = 'Read More ▼';
            }
        }
    
        // Call this function on page load
        window.onload = checkReadMoreVisibility;
    </script>
    
    
         -->
            <script>
                function showPremiumOverlay() {
                    document.getElementById('premiumOverlay').style.display = 'flex';
                }

                document.getElementById('closeOverlay').addEventListener('click', function () {
                    document.getElementById('premiumOverlay').style.display = 'none';
                });
            </script>


            <script>

                document.addEventListener('DOMContentLoaded', () => {
                    const navBoxes = document.querySelectorAll('.nav-box');
                    const mainContents = document.querySelectorAll('.main-content');
                    const chapterHeaders = document.querySelectorAll('.chapter-header');
                    const faqQuestions = document.querySelectorAll('.faq-question');
                    const codeBackground = document.getElementById('codeBackground');
                    const ctaButton = document.getElementById('ctaButton');
                    const premiumOverlay = document.getElementById('premiumOverlay');
                    const closeOverlay = document.getElementById('closeOverlay');

                    // Navigation
                    navBoxes.forEach(box => {
                        box.addEventListener('click', () => {
                            const contentId = box.getAttribute('data-content');
                            mainContents.forEach(content => {
                                content.classList.remove('active');
                            });
                            document.getElementById(contentId).classList.add('active');
                        });
                    });

                    // Chapter toggles
                    chapterHeaders.forEach(header => {
                        header.addEventListener('click', () => {
                            const content = header.nextElementSibling;
                            const toggle = header.querySelector('.chapter-toggle');
                            if (content.style.display === 'block') {
                                content.style.display = 'none';
                                toggle.textContent = '▼';
                            } else {
                                content.style.display = 'block';
                                toggle.textContent = '▲';
                            }
                        });
                    });

                    // FAQ toggles
                    faqQuestions.forEach(question => {
                        question.addEventListener('click', () => {
                            const answer = question.nextElementSibling;
                            answer.style.display = answer.style.display === 'block' ? 'none' : 'block';
                        });
                    });

                    // Premium Overlay functionality
                    ctaButton.addEventListener('click', () => {
                        premiumOverlay.style.display = 'flex';
                    });

                    closeOverlay.addEventListener('click', () => {
                        premiumOverlay.style.display = 'none';
                    });

                    // Animated code background
                    const codeSnippets = [
                        'function designSystem() {',
                        '  const scalability = implement(horizontalScaling);',
                        '  const reliability = ensure(faultTolerance);',
                        '  const performance = optimize(caching, loadBalancing);',
                        '  return robustSystem(scalability, reliability, performance);',
                        '}',
                        'class DistributedSystem {',
                        '  constructor(nodes, network) {',
                        '    this.nodes = nodes;',
                        '    this.network = network;',
                        '  }',
                        '  coordinate() {',
                        '    // Implementation details',
                        '  }',
                        '}',
                        'async function handleRequest(req, res) {',
                        '  const data = await fetchFromDatabase(req.query);',
                        '  const cachedResult = checkCache(data);',
                        '  if (cachedResult) return cachedResult;',
                        '  const processedData = processData(data);',
                        '  cacheResult(processedData);',
                        '  return res.json(processedData);',
                        '}'
                    ];

                    function animateBackground() {
                        let content = '';
                        for (let i = 0; i < 50; i++) {
                            content += codeSnippets[Math.floor(Math.random() * codeSnippets.length)] + '<br>';
                        }
                        codeBackground.innerHTML = content;
                        gsap.fromTo(codeBackground,
                            { y: '0%' },
                            { y: '-50%', duration: 10, ease: 'linear', repeat: -1, yoyo: true }
                        );
                    }

                    animateBackground();
                });
            </script>

            <script>
                document.addEventListener('DOMContentLoaded', function () {
                    // Get all item cards
                    const itemCards = document.querySelectorAll('.item-card');

                    // Add click event listener to each item card
                    itemCards.forEach(card => {
                        card.addEventListener('click', function () {
                            // Get the target id from the data attribute
                            const targetId = card.getAttribute('data-target');
                            // Scroll to the target element
                            const targetElement = document.getElementById(targetId);
                            if (targetElement) {
                                targetElement.scrollIntoView({
                                    behavior: 'smooth'
                                });
                            }
                        });
                    });
                });
            </script>

            <script>
                function scrollToSection() {
                    document.getElementById("control-structure").scrollIntoView({
                        behavior: "smooth"
                    });
                }
            </script>

            <script>
                function scrollToList() {
                    document.getElementById("list-section").scrollIntoView({
                        behavior: "smooth"
                    });
                }
            </script>

            <script>
                function scrollToFunctions() {
                    document.getElementById("functions-section").scrollIntoView({
                        behavior: "smooth"
                    });
                }
            </script>

            <script>
                function scrollToObjects() {
                    document.getElementById("objects-section").scrollIntoView({
                        behavior: "smooth"
                    });
                }

                function scrollToModularDesign() {
                    document.getElementById("modular-design-section").scrollIntoView({
                        behavior: "smooth"
                    });
                }

                function scrollToTextFiles() {
                    document.getElementById("text-files-section").scrollIntoView({
                        behavior: "smooth"
                    });
                }

                function scrollToDictionaries() {
                    document.getElementById("dictionaries-section").scrollIntoView({
                        behavior: "smooth"
                    });
                }

                function scrollToOOP() {
                    document.getElementById("oop-section").scrollIntoView({
                        behavior: "smooth"
                    });
                }

                function scrollToRecursion() {
                    document.getElementById("recursion-section").scrollIntoView({
                        behavior: "smooth"
                    });
                }
            </script>
<script>
document.addEventListener('DOMContentLoaded', () => {
    const faqQuestions = document.querySelectorAll('.faq-question');
    
    faqQuestions.forEach(question => {
        question.addEventListener('click', () => {
            const answer = question.nextElementSibling;
            answer.style.display = answer.style.display === 'block' ? 'none' : 'block';
        });
    });
});
</script>



    </body>

</html>